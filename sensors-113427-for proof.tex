% LaTeX support: latex@mdpi.com
% In case you need support, please attach any log files that you could have, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

%=================================================================

% LaTeX Class File and Rendering Mode (choose one)
% You will need to save the "mdpi.cls" and "mdpi.bst" files into the same folder as this template file.

%=================================================================

\documentclass[sensors,article,accept,moreauthors,pdftex,10pt,a4paper]{mdpi}
%--------------------
% Class Options:
%--------------------
% journal
%----------
% Choose between the following MDPI journals:
% actuators, administrativesciences, aerospace, agriculture, agronomy, algorithms, animals, antibiotics, antibodies, antioxidants, appliedsciences, arts, atmosphere, atoms, axioms, batteries, behavioralsciences, bioengineering, biology, biomedicines, biomimetics, biomolecules, biosensors, brainsciences, buildings, cancers, catalysts, cells, challenges, chemosensors, children, chromatography, climate, coatings, computation, computers, cosmetics, crystals, data, dedntistryjournal, diagnostics, diseases, diversity, econometrics, economies, education, electronics, energies, entropy, environmentalsciences, environments, epigenomes, fibers, foods, forests, futureinternet, galaxies, games, gels, genealogy, genes, geosciences, healthcare, horticulturae, humanities, hydrology, informatics, information, inorganics, insects, ijerph, ijfs, ijms, ijgi, jcdd, jcm, jdb, jfb, jimaging, jof, joi, jlpea, jmse, jpm, jrfm, jsan, land, laws, life, lubricants, machines, marinedrugs, materials, mathematics, medicalsciences, membranes, metabolites, metals, microarrays, micromachines, microorganisms, minerals, molbank, molecules, nanomaterials, ncrna, nutrients, pathogens, pharmaceuticals, pharmaceutics, pharmacy, photonics, plants, polymers, processes, proteomes, publications, religions, remotesensing, resources, risks, robotics, safety, sensors, sinusitis, socialsciences, societies, sports, standards, sustainability, symmetry, systems, technologies, toxics, toxins, universe, vaccines, veterinarysciences, viruses, water
%---------
% article
%---------
% The default type of manuscript is article, but could be replaced by using one of the class options:
% article, review, communication, commentary, bookreview, correction, addendum, editorial, changes, supfile, casereport, comment, conceptpaper, conferencereport, meetingreport, discussion, essay, letter, newbookreceived, opinion, projectreport, reply, retraction, shortnote, technicalnote, creative, datadescriptor (for journal Data)
%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g. the logo of the journal will get visible), the headings, and the copyright information. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.
% Please insert a blank line is before and after all equation and eqnarray environments to ensure proper line numbering when option submit is chosen
%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.
%---------
% pdftex
%---------
% The option "pdftex" is for use with pdfLaTeX only. If eps figure are used, use the optioin "dvipdfm", with LaTeX and dvi2pdf only.

%=================================================================
\firstpage{1}
\makeatletter %%NEW LAYOUT%%
\setcounter{page}{\@firstpage} %%NEW LAYOUT%%
\makeatother %%NEW LAYOUT%%

\articlenumber{x}

\doinum{10.3390---}
\pubvolume{16}
\pubyear{2016}
\copyrightyear{2016}
\externaleditor{Academic Editor: Vittorio M.N. Passaro}
\history{Received: 20 December 2015; Accepted: 29 February 2016; Published: }
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1
\usepackage{setspace}
%=================================================================
\usepackage{soul}
\usepackage{upgreek}
\usepackage{enumitem}
%\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{microtype}





% Add packages and commands to include here
% The amsmath, amsthm, amssymb, hyperref, caption, float and color packages are loaded by the MDPI class.
\usepackage{graphicx}
\usepackage[caption=false]{subfig}
%\usepackage{subfigure,psfig}

\usepackage[utf8]{inputenc} % for use umulaut
\usepackage{textcomp}	% for use Trademarks symbol

\usepackage{booktabs}
\newcommand{\bigcell}[1]{\shortstack[l]{#1}}

\usepackage{multirow,tabularx}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
\renewcommand{\algorithmiccomment}[1]{\hfill\bgroup//~#1\egroup}
\algnewcommand{\LineComment}[1]{\State // #1}

\newcommand{\JHMEMO}[1]{\textcolor{red}{#1}}

\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}


\usepackage{colortbl}

% \setcounter{secnumdepth}{5}

%=================================================================
%% Please use the following mathematics environments:
% \theoremstyle{mdpi}
% \newcounter{thm}
% \setcounter{thm}{0}
% \newcounter{ex}
% \setcounter{ex}{0}
% \newcounter{re}
% \setcounter{re}{0}
%
% \newtheorem{Theorem}[thm]{Theorem}
% \newtheorem{Lemma}[thm]{Lemma}
% \newtheorem{Corollary}[thm]{Corollary}
% \newtheorem{Proposition}[thm]{Proposition}
%
% \theoremstyle{mdpidefinition}
% \newtheorem{Characterization}[thm]{Characterization}
% \newtheorem{Property}[thm]{Property}
% \newtheorem{Problem}[thm]{Problem}
% \newtheorem{Example}[ex]{Example}
% \newtheorem{ExamplesandDefinitions}[ex]{Examples and Definitions}
% \newtheorem{Remark}[re]{Remark}
% \newtheorem{Definition}[thm]{Definition}
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================

% Full title of the paper (Capitalized)
\Title{Fast Contour-Tracing Algorithm Based on a%please note the correction to the title
 Pixel-Following Method for Image Sensors}

% Authors (Add full first names)
\Author{Jonghoon Seo $^{1}$, Seungho Chae $^{2}$, Jinwook Shim $^{2}$, Dongchul Kim $^{2}$, Cheolho Cheong $^{2}$ and~Tack-Don Han $^{2,}$*}
%Please carefully check the accuracy of names and affiliations. Changes will not be possible after proofreading.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
	$^{1}$ \quad Software Platform R\&D Lab., LG Electronics Advanced Research Institute, 19 Yangjae-daero 11 gil, Seocho-gu, Seoul 137-893, Korea;
 jonghoon.seo@lge.com \\
	$^{2}$ \quad Department of Computer Science, Yonsei University, 134 Sinchon-dong Seodaemun-gu, \hl{Seoul postcode}, Korea;
seungho.chae@msl.yonsei.ac.kr (S.C.); jin99foryou@msl.yonsei.ac.kr (J.S.); \linebreak dckim@msl.yonsei.ac.kr (D.K.); balgeum00@msl.yonsei.ac.kr (C.C.)
}%Please add post code. 

% \contributed{$^\dagger$ These authors contributed equally to this work.}

% Contact information of the corresponding author (Add [2] after \corres if there are more than one corresponding author.)
\corres{Correspondence: hantack55@msl.yonsei.ac.kr; Tel.: +82-2-2123-2715; Fax: +82-2-365-2579}

% Abstract (Do not use inserted blank lines, i.e. \\)
% \abstract{This is the abstract section. The abstract should be one section and count less than 200 words.}
%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

% \abstract{In this paper, we present a novel contour tracing algorithm based on the pixel following method for fast and accurate contour following. The proposed algorithm distinguishes between types of contour such as a local pattern type based on its relative location among several contour pixels, and then it traces the next contour pixel by using the previous. Therefore, it can classify the type of contour pixels into straight line, inner corner, outer corner, and inner-outer corner, and it can extract pixels of a specific contour type. Moreover, it can trace contour pixels rapidly because it can determine the local minimal path using the contour case. In addition, the proposed algorithm has the ability to compress data of contour pixels by using the representative points and inner-outer corner points, and it can also accurately restore the contour image from the data. To compare the performance of the proposed algorithm with that of conventional techniques, we measured their processing time and accuracy. In the experimental results, the proposed algorithm shows better performance when compared to the others, and it can provide compressed data of contour pixels and restore them accurately, including the inner-outer corner that cannot be restored using conventional algorithms.}

\abstract{
Contour pixels distinguish objects from the background. Tracing and extracting contour pixels are widely used for smart/wearable image sensor devices, because these are simple and useful for detecting objects. In this paper, we present a novel contour-tracing algorithm for fast and accurate contour following. 
The proposed algorithm classifies the type of contour pixel, based on its local pattern. Then, it traces the next contour using the previous pixel's type. Therefore, it can classify the type of contour pixels as a straight line, inner corner, outer corner and inner-outer corner, and it can extract pixels of a specific contour type. Moreover, it can trace contour pixels rapidly because it can determine the local minimal path using the contour case. In addition, the proposed algorithm is capable of the compressing data of contour pixels using the representative points and inner-outer corner points, and it can accurately restore the contour image from the data. To compare the performance of the proposed algorithm to that of conventional techniques, we measure their processing time and accuracy. In the experimental results, the proposed algorithm shows better performance compared to the others. Furthermore, it can provide the compressed data of contour pixels and restore them accurately, including the inner-outer corner, which cannot be restored using conventional algorithms.}



% Keywords: add 3 to 10 keywords
\keyword{contour tracing; boundary following; pixel following; contour data compression}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{}
%\MSC{}
%\JEL{}

% If this is an expanded version of a conference paper, please cite it here: enter the full citation of your conference paper, and add $^\dagger$ in the end of the title of this article.
%\conference{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% For journal Data:

%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}
%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

\section{Introduction}

% A contour is defined as a segment that is one pixel wide and one or more pixels in length, and a boundary is defined as an unbroken contour\cite{Mcqueen2004Contour}. Contours and boundaries are very important information for object representation and image recognition. For example, they are used to separate objects from the background, to calculate the size of an object, to classify a shape, and to find the feature points of an object using the length and shape from its contour pixels\cite{Pratt????Digital,Gose1996Pattern}. Moreover, using the contour information, it is possible to save the shape of objects and restore them to their original shapes for various applications in the field of graphics and vision. Therefore, many researches on contour tracing algorithms for extracting and tracing the contour of an object have been conducted. Most of the algorithms are binary-image-based contour tracing algorithms \cite{Pitas2000Digital,Gose1996Pattern,Das1990Bivariate,Papert1973Uses,Cheong2006Improved,Mirante1982Radial,Pavlidis2012Algorithms}, which trace contours on digitized black-and-white images. Moreover, the focus is on contour tracing algorithms based on color images and gray images []; further, the binary-image-based contour tracing algorithm can be easily applied to color and gray images.

A contour is defined as a segment that is one pixel wide and one or more pixels in length, and a boundary is defined as an unbroken contour \cite{Mcqueen2004Contour}. Contours and boundaries provide very important information for object representation and image recognition. For example, they are used to separate objects from their backgrounds, to calculate the sizes of objects, to classify shapes and to find the feature points of objects using the length and shape of their contour pixels \cite{Pratt????Digital,Gose1996Pattern}. Moreover, in the field of graphics and vision, it is possible to use the contour information to save the shape of objects and restore them to their original shapes for various applications. Therefore, there have been many studies on contour-tracing algorithms for extracting and tracing the contour of an object. Most of the algorithms are binary image-based contour-tracing algorithms \cite{Pitas2000Digital,Gose1996Pattern,Das1990Bivariate,Papert1973Uses,Cheong2006Improved,Mirante1982Radial,Pavlidis2012Algorithms}, which trace contours on digitized black and white images taken from various image sensors.

% In recent years, as the popularity of camera equipped mobile devices such as mobile phones and personal digital assistants (PDAs) has increased, various real-time applications such as image code recognition, face recognition, optical character recognition (OCR), logo recognition, augmented reality (AR), and mixed reality (MR) have emerged for mobile computing environments [], [], []. Since mobile devices possess restricted hardware resources such as low-performance processors, small-sized memory, low-resolution display, and low battery capacity, that require simpler and faster algorithms for image processing. 

In recent years, with the increasing popularity of smart/wearable mobile sensor devices \cite{Aroca2013Wearable}, such as smart phones, smart watches and smart glasses, various real-time applications, such as image code recognition, face recognition, optical character recognition (OCR), logo recognition, augmented reality (AR) and mixed reality (MR), have emerged for sensor computing~\cite{Wakaumi20062D,Brodic2010Basic,Kim2006Rapid,Tian2010RealTime,Zhang2012Robust}. Because smart/wearable mobile sensor devices possess limited hardware resources, such as low-performance processors, small-sized memory, low-resolution displays and low battery capacity, they require simple and fast algorithms for image processing. 

% Generally, a contour tracing algorithm can be evaluated on the basis of the following four criteria: (1) accuracy of contour tracing, (2) processing time, (3) data size to save the traced contour information, and (4) the ability to accurately restore and enlarge the original contour using the saved data. However, there have been few studies on contour tracing algorithms that have sought to satisfy all these criteria. Some of the conventional algorithms miss contour pixels on specific relative pixel locations, and the others require considerable operation time for tracing the pixels because a shortcut to the local patterns is not considered \cite{Cheong2006Improved,Cheong2012Advanced}. Moreover, most of the algorithms have no data compression ability for saving the contour information, and some of them cannot restore the image perfectly using the saved data \cite{Miyatake1997Contour}. 

Generally, a contour tracing algorithm can be evaluated based on the following four criteria: (1)~the accuracy of contour tracing; (2) processing time; (3) data size to save the traced contour information; and (4) the ability to accurately restore and enlarge the original contour using the saved data. However, few studies on contour-tracing algorithms have sought to satisfy all of these criteria. Some of the conventional algorithms miss contour pixels that are at specific relative pixel locations, and others require considerable processing time to trace the pixels, because shortcuts to the local patterns are not considered \cite{Cheong2006Improved,Cheong2012Advanced}. Moreover, most of the algorithms have no data-compression capabilities that enable them to save the contour information, and some of them cannot restore the contour perfectly using the saved data~\cite{Miyatake1997Contour}. 

% In this paper, we propose a novel contour tracing algorithm based on pixel following that overcomes the abovementioned problems, i.e., (1) it provides fast and accurate results for contour pixel tracing, (2) contour information can be compressed for reducing the memory size, and (3) it accurately restores the compressed data to the original contour image. In order to achieve the objectives, the proposed algorithm initially distinguishes the local patterns made by adjacent contour pixels, then finds next contour pixel will be traced from the pattern. 

In this paper, we propose a novel contour-tracing algorithm based on pixel following that overcomes the above-mentioned problems, \emph{i.e.},: (1) it provides fast and accurate results for contour-pixel tracing; (2) contour information can be compressed to reduce the memory size; and (3) it accurately restores the compressed data to the original contour image. In order to achieve the objectives, the proposed algorithm initially distinguishes the local patterns made by adjacent contour pixels, and it then finds the next contour pixel that will be traced from the pattern. 

% The paper is organized as follows. In the next section, conventional contour tracing algorithms are categorized and their characteristics are introduced. Subsequently, we analyze their performance on the basis of accuracy and speed of the contour tracing process, and 
%then the proposed algorithm is described and its contour tracing procedure, data compression technique, and restoration technique are presented. After that, the comparison of the conventional algorithms and the proposed algorithm on the basis of performance is presented along with experimental results that comprise the number of traced pixels and the processing times for real-time large-sized images. Moreover, the compressed data size and its restored results are compared with the original traced contour pixels. Finally, we summarize the characteristics and experimental results of the proposed algorithm.

The paper is organized as follows. In Section 2, we categorize conventional contour-tracing algorithms and introduce their characteristics. Then, in Section 3, we analyze their performance based on the accuracy and speed. Subsequently, in Section 4, the proposed algorithm is described, and its contour tracing procedure, contour data compression technique and restoration technique are presented. In Section 5, we present a comparison of the conventional algorithms and the proposed algorithm on the basis of their performance, along with experimental results that include the number of traced pixels and the processing times for real-time large-sized images. Moreover, we compare the compressed data size and its restored results with the original traced contour pixels. Finally, in \linebreak Section 6, we summarize the characteristics and experimental results of the proposed algorithm.

%-----------------------------------------------------------------------------------------
%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

\section{Related Works}

% Most conventional contour tracing algorithms run on two-dimensional (2D) binary images that consist of black-and-white pixels, which are obtained through binarization, for determining whether a pixel has dark or light intensity. In the digitized image, we can assume that the objects have black pixels and the background has white pixels; therefore, a contour pixel is black and it has at least one adjacent pixel that is white.

To determine whether a pixel has a dark or light intensity, most conventional contour-tracing algorithms process using 2D binary images that consist of black and white pixels through binarization. In the digitized image, we can assume that the objects have black pixels and that the background has white pixels; therefore, a contour pixel is black, and it has at least one adjacent pixel that is white.

\subsection{Overview}

Figure \ref{fig:image1} shows examples of contour traces that were obtained using the contour-tracing algorithms. The conventional contour algorithms can typically be categorized into three types as follows: pixel following, vertex following and run-data-based following \cite{Miyatake1997Contour,Danielsson1981Improvement,Shoji1999Contour}. Of these, the pixel-following method is the most common.

\subsubsection{Pixel-Following Method}

% The pixel following method traces contour pixels in a predefined manner and then saves their coordinates in the memory according to the trace order. In figure \ref{fig:img1-a}, the algorithm traces contour pixels along the clockwise direction from the current pixel, i.e., it sequentially searches adjacent black pixels of the current pixel using a relative directional order such as left, front-left, front, front-right, right, right-rear, and rear. Pixel following methods such as simple boundary follower (SBF) \cite{Pitas2000Digital,Das1990Bivariate,Papert1973Uses}, modified SBF (MSBF)\cite{Gose1996Pattern}, improved SBF (ISBF)\cite{Cheong2006Improved}, Moore-Neighbor tracing (MNT) \cite{Toussaint????Grids}, modified MNT (MMNT) \cite{Pradhan2010Contour}, radial sweep algorithm (RSA) \cite{Mirante1982Radial}, and Theo Pavlidis algorithm (TPA)\cite{Pavlidis2012Algorithms} have simple rules for tracing contour pixels based on a chain code. On the contrary, this method requires frame size memory to trace the contour, and it also generates erroneous images when the contour image is enlarged\cite{Miyatake1997Contour} since it maintains only the pixel coordinates.

The pixel-following method traces contour pixels in a predefined manner and then saves their coordinates in memory according to the trace order. In Figure \ref{fig:img1-a}, the algorithm traces contour pixels along the clockwise direction from the current pixel, \emph{i.e.}, it sequentially searches adjacent black pixels of the current pixel using a relative directional order, such as left, front-left, front, front-right, right, rear-right and rear. Pixel-following methods, such as the simple boundary follower (SBF) \cite{Pitas2000Digital,Das1990Bivariate,Papert1973Uses}, modified SBF (MSBF) \cite{Gose1996Pattern}, improved SBF (ISBF) \cite{Cheong2006Improved}, Moore-neighbor tracing (MNT) \cite{Toussaint????Grids}, the radial sweep algorithm (RSA) \cite{Mirante1982Radial} and the Theo Pavlidis algorithm (TPA) \cite{Pavlidis2012Algorithms}, have simple rules for tracing contour pixels based on a chain code. These methods require a frame-size memory to trace the contour and generate erroneous images when the contour image is enlarged \cite{Miyatake1997Contour} because they maintain only the pixel coordinates.
%%% Image 1
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.3\textwidth, height=5cm]{2.RelatedWorks/fig1-a.png} \label{fig:img1-a} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth, height=5cm]{2.RelatedWorks/fig1-b.png} \label{fig:img1-b} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth, height=5cm]{2.RelatedWorks/fig1-c.png} \label{fig:img1-c} }
	 
	\caption{Category of contour tracing algorithms. (\textbf{a}) Pixel-following algorithm; (\textbf{b}) vertex-following algorithm; (\textbf{c}) run-data-based following algorithm.}
	\label{fig:image1}
\end{figure}

% Figure \ref{fig:image1} shows examples of contour traces obtained by using the contour tracing algorithms. The conventional contour algorithms can be categorized typically into three types as follows: pixel following, vertex following, and run-data-based following \cite{Miyatake1997Contour,Danielsson1981Improvement,Shoji1999Contour}. Among these, the pixel following method is most the common.




\subsubsection{Vertex-Following Method}
% The vertex following method traces the vertices of the contour pixels that are located on the edges between the contour pixels and the background pixels \cite{Miyatake1997Contour}. Its procedure is similar to that of the pixel following method since it uses a chain code and requires a frame size memory for contour tracing; however, it traces the corners of the contour pixels and their connected edges. Moreover, it stores the corner points of the contour pixels in order to save the traced contour information and the data can be compressed by reducing the number of points in a straight line. For example, in figure \ref{fig:img1-b}, five points of the contour from s to t can be stored as just two points such as (2.5, 2.5) and (6.5, 2.5) based on the (x, y) coordinate system. Moreover, when the contour images are enlarged, the vertex following method can provide the correct image \cite{Miyatake1997Contour} since the traced points form the boundaries of the contour.

The vertex-following method traces the vertices of the contour pixels that are located on the edges between the contour pixels and the background pixels \cite{Miyatake1997Contour}. Its procedure is similar to that of the pixel-following method because it uses a chain code and requires a frame-size memory for contour tracing; however, it traces the corners of the contour pixels and their connected edges. Moreover, it stores the corner points of the contour pixels in order to save the traced contour information, and the data can be compressed by reducing the number of points in a straight line. For example, in Figure \ref{fig:img1-b}, five points of the contour from s to t can be stored as there are only two corner points, \emph{i.e.}, $(2.5, 2.5)$ and $(6.5, 2.5)$ based on the $(x, y)$ coordinate system. Moreover, when the contour images are enlarged, the vertex-following method can provide the correct image \cite{Miyatake1997Contour} because the traced points form the boundaries of the contour.

\subsubsection{Run-Data-Based Following Method}
% The run-data-based following method, involving the edge-point tracing of rundata \cite{Miyatake1997Contour,Shoji1999Contour}, uses run data in pairs consisting of the left and right edges of an object obtained by horizontal scan lines from left to right on an image. The object can have an outer contour and additional inner contours; therefore, the run data has five types: (left edge of outer contour, right edge of outer contour), (left edge of outer contour, left edge of inner contour), (right edge of inner contour, left edge of inner contour), (right edge of inner contour, right edge of outer contour), and (right edge of outer contour, left edge of outer contour). For contour following, the run-data-based following method constructs a run-following relationship between the edge points of two adjacent scan lines. In figure \ref{fig:img1-c}, scan line \#3 detects (left edge of 5, right edge of 6) and scan line \#4 detects (left edge of 7, right edge of 8), and subsequently the run-following relationships between 5 and 7 and between 8 and 6 are generated. The method uses only one or two line buffers; therefore, it requires a smaller amount of memory as compared to the pixel following and vertex following methods since it uses only one or two scan lines. Examples of the method are run-type direction code (RD code) method \cite{Miyatake1997Contour} and PXY table based method \cite{Shoji1999Contour}. 

The run-data-based following method, which involves the edge-point tracing of run data \cite{Miyatake1997Contour,Shoji1999Contour}, uses run data in pairs consisting of an object's left and right edges, which are obtained using horizontal scan lines from left to right on an image. The object can have an outer contour and additional inner contours. Therefore, there are five types of run data: \hl{(}%please check the use of () here; please check the use of the () for these data conventions throughout
left edge of outer contour, right edge of outer contour), (left edge of outer contour, left edge of inner contour), (right edge of inner contour, left edge of inner contour), (right edge of inner contour, right edge of outer contour) and (right edge of outer contour, left edge of outer contour). For contour following, the run-data-based following method constructs a run-following relationship between the edge points of two adjacent scan lines. In~\mbox{Figure \ref{fig:img1-c}}, Scan Line \#3 detects \hl{(}left edge of 5%is this a whole number? if so, write out as five
, right edge of 6%see the previous note
), and Scan Line \#4 detects (left edge of 7%see the previous note
, right edge of 8%see the previous note
). Subsequently, the run-following relationships between 5 and 7 and between 8 and 6 are generated. The method uses only one or two line buffers and therefore requires a smaller amount of memory compared to the pixel-following and vertex-following methods. Examples of this method are the run-type direction code (RD code) method \cite{Miyatake1997Contour}, the \hl{PXY}
 table-based method \cite{Shoji1999Contour} and the OpenCV method \cite{Suzuki1985Topological}. 

Table \ref{table:relatedworks} lists the characteristics of the contour-following methods. The pixel-following method and vertex-following method trace contours without scanning all of the pixels of the image, and their transition data, such as contour points and the tracing sequence, are generated automatically by the contour-following process. Therefore, only a few pixels need to be scanned in order to obtain the first contour pixel representing the starting point of the object. Despite these merits, they are not suitable for large images with many objects because they require more operations and a larger memory when compared to the run-data-based following method. In other words, they scan all of the pixels with an image-buffer size memory in order to obtain all of the objects, and they have several additional operations that enable them to detect whether to follow a contour pixel for all of the contour pixels and their adjacent background pixels (see \hl{Figure} \ref{fig:image8}). 


\begin{table}[H]
\footnotesize
\centering
	% \begin{tabular}{l|lll}
	\begin{tabular}{C{2cm}C{3.5cm}C{3.5cm}C{4cm}}
		\toprule
		& \textbf{Pixel-Following \linebreak Method} & \textbf{Vertex-Following \linebreak Method} & \textbf{Run-Data-Based Following~Method} \\
		\midrule
		Traced object 		& Contour pixel & Vertex of contour (pixel~corner) & Run-data \\		\midrule
		Data construction 	& Coordinates of contour pixels obtained using traced\ sequence (automatically) 
 & Coordinates of vertices of contour pixels obtained using traced sequence (automatically)
 & All run-data of image and run-following relationship data 
(additive operation for calculating the relationship between adjacent run-data~horizontally) \\		\midrule
		Adaptive application \cite{Miyatake1997Contour}	& Small-scale image \mbox{Slow trace is allowed}
	& Small-scale image \mbox{Slow trace is allowed}	& Large-scale image, such as \mbox{document recognition} \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of contour-following algorithms.}
	\label{table:relatedworks}
\end{table}

% Table \ref{table:relatedworks} lists the characteristics of the contour following methods. The pixel following method and vertex following method trace contours without scanning all the pixels of the image and their transition data such as contour points and the tracing sequence are generated automatically by the contour following process. Therefore, they need not scan all the pixels but only a few pixels for obtaining the first contour pixel for the starting point of the object. Despite these merits, they are not suitable for large images with many objects since they require more operations and a larger memory as compared to the run-data-based following method, i.e., they scan all the pixels with an image buffer size memory to obtain all the objects and have several additional operations to detect whether or not to follow a contour pixel for all the contour pixels and their adjacent background pixels (see \JHMEMO{figure 8}). 


% On the contrary, the run-data-based following method searches the edge points with a smaller memory and constructs run-following relationships between edge points. Therefore, the traced run-following results are changed iteratively while the edge points are updated. The method is not simple but it is faster than the other methods in the case of a large-scale image since it scans all the pixels once, and it does not require any additional operations to be carried out on the pixels. Hence, it is suitable for large-scale-image-based applications involving a large number of objects such as document recognition \cite{Miyatake1997Contour}.

On the contrary, the run-data-based following method searches the edge points with a smaller memory and constructs run-following relationships between edge points. Therefore, the traced run-following results are changed iteratively while the edge points are updated. This method is not simple, but it is faster than the other methods for large-scale images, because it scans all of the pixels once; and it does not require any additional operations to be carried out on the pixels. Hence, it is suitable for large-scale image-based applications involving a large number of objects, such as document recognition \cite{Miyatake1997Contour}.

\subsection{Conventional Contour Tracing Algorithms}

% Let $I$ be a binary digital image with $M \times N$ pixels where the coordinate of the top-left-most pixel is $(0, 0)$ and that of the bottom-right-most pixel is $(M-1, N-1)$. In $I$, a pixel can be represented as $P = (x, y),\, x = 0,1,2,\cdots,M-1,\, y = 0,1,2,\cdots, N-1$. Most contour tracing algorithms uses a tracer $T (P, d)$ with absolute directional information $d\in\{N,NE,NW,W,SW,S,SE,E,NE\}$ , and they have the following basic sequence: 

Let I
 be a binary digital image with $M \times N$ pixels, where the coordinate of the top-leftmost pixel is $(0, 0)$ and that of the bottom-rightmost pixel is $(M - 1, N - 1)$. In $I$, a pixel can be represented as $P = (x, y),\, x = 0,1,2,\cdots,M-1,\, y = 0,1,2,\cdots, N-1$. Most contour-tracing algorithms use a tracer $T (P, d)$ with absolute directional information $d\in\{N,NE,NW,W,SW,S,SE,E,NE\}$, and they have the following basic sequence: 

% \begin{enumerate}
% \item The tracer starts contour tracing at a contour of an object after it saves the starting point along with its initial direction. 
% \item The tracer determines the next contour point using its peculiar rule of following paths on the basis of the adjacent pixels and then moves to the contour point and changes its absolute direction. 
% \item If the tracer reaches the start point then the trace procedure is terminated. 
% \end{enumerate}

\begin{enumerate}[leftmargin=*,labelsep=5mm]
\item The tracer starts contour tracing at the contour of an object after it saves the starting point along with its initial direction. 
\item The tracer determines the next contour point using its specific rule of following paths according to the adjacent pixels and then moves to the contour point and changes its absolute direction.
\item If the tracer reaches the start point, then the trace procedure is terminated. 
\end{enumerate}


% For determining the next contour point, which could be a contour pixel or pixel corner, the tracer detects the intensity of its adjacent pixel $P_r$ and the new absolute direction $d_r$ for $P_r$ by using relative direction information $r\in\{front,\ front-left,\ left,\ left-rear,\ rear,\ right-rear,\ right,\ front-right\}$. For example, if the absolute direction of the current tracer $T(P, d)$ is $N$, the left direction of the tracer $d_{Left}$ is $W$. Similarly, the left pixel of tracer $P_{Left}$ is $(x-1, y)$. Figures \ref{fig:img2-a} and \ref{fig:img2-b} show the directional information of the tracer and figure \ref{fig:img2-c} shows the types of contour pixels. The contour pixels can be classified into four types, namely, straight line, inner corner pixel, outer corner pixel, and inner-outer corner pixel. In figure \ref{fig:img2-c}, ``O'' represents the outer corner; ``I'' the inner corner; and ``IO'' the inner-outer corner according to the local pattern of the contour.

To determine the next contour point, which may be a contour pixel or pixel corner, the tracer detects the intensity of its adjacent pixel $P_r$ and the new absolute direction $d_r$ for $P_r$ by using relative direction information $r\in\{front,\ front-left,\ left,\ rear-left,\ rear,\ rear-right,\ right,\ front-right\}$. For example, if the absolute direction of the current tracer $T(P, d)$ is $N$, the left direction of the tracer $d_{Left}$ is $W$. Similarly, the left pixel of tracer $P_{Left}$ is $(x - 1, y)$. Figure \ref{fig:image2}a,b show the directional information of the tracer, and Figure \ref{fig:img2-c} shows the different types of contour pixels. The contour pixels can be classified into four types, namely straight line, inner corner pixel, outer corner pixel and inner-outer corner pixel. In Figure \ref{fig:img2-c}, ``$O$'' represents the outer corner, ``$I$'' represents the inner corner and ``$IO$'' represents the inner-outer corner according to the local pattern of the contour.

% In this study, we focus on a contour tracing algorithm that is suitable for situations involving a relatively small number of objects and requiring real-time tracing such as AR, MR, and recognition image-based code in small scale images, e.g., the mobile computing environment. Hence, we first introduce and briefly describe the conventional contour tracing algorithms that are used in this environment and analyze their tracing accuracy and characteristics. 

In this study, we focus on a contour-tracing algorithm that is suitable for cases involving a relatively small number of objects and that require real-time tracing, such as augmented reality (AR), mixed reality (MR) and recognition image-based code in small-scale images, e.g., a mobile computing environment. Hence, we first introduce and briefly describe the conventional contour-tracing algorithms that are used in this environment and analyze their tracing accuracy and characteristics. 

%%% Image 2
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.32\textwidth, height=5cm]{2.RelatedWorks/fig2-a.png} \label{fig:img2-a} }
	\subfloat[]{ \includegraphics[width=0.33\textwidth, height=5cm]{2.RelatedWorks/fig2-b.png} \label{fig:img2-b} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth, height=5cm]{2.RelatedWorks/fig2-c.png} \label{fig:img2-c} }
	 
	\caption{Directions and types of contour pixels. (\textbf{a}) Absolute direction $d$; (\textbf{b}) relative direction $r$; (\textbf{c})~types of contour pixels: inner corner pixel ($I$), outer corner pixel ($O$) and inner-outer corner pixel~($IO$).}
	\label{fig:image2}
\end{figure}


\subsubsection{Simple Boundary Follower}

% SBF is also known as Papert's turtle algorithm \cite{Das1990Bivariate} and as square tracing algorithm\cite{Toussaint????Grids}, and it is the simplest contour tracing algorithm. Initially, the location of tracer $S$ is saved, and the tracer moves leftward or rightward; if the pixel tracer is located on a contour pixel, the tracer moves leftward, otherwise it moves rightward. The procedure is as given below.

The simple boundary follower (SBF) is also known as Papert's turtle algorithm \cite{Papert1973Uses} and as a square-tracing algorithm \cite{Ghuneim2015Contour}, and it is the simplest contour-tracing algorithm. Initially, the location of tracer $S$ is saved, and the tracer moves in a left or right direction. If the pixel tracer is located on a contour pixel, the tracer moves left; otherwise, it moves right. The procedure is as given \hl{below}.

\begin{algorithm}[H]
	\caption{Algorithm of the simple boundary follower.}\label{alg:sbf}
	\begin{algorithmic}[1]
	\begin{spacing}{1.5}
	\Procedure{SBF}{}
	\State $\textit{T(P,d)} \gets \textit{S(P,d)}$
	\Do
	\If {$\textit{P}$ = black}
	\State $\textit{T(P,d)} \gets \textit{T(P}_{Left},\textit{d}_{Left} ) $
	\Else
	\State $\textit{T(P,d)} \gets \textit{T(P}_{Right},\textit{d}_{Right})$
	\EndIf
	\doWhile {$ \textit{T(P,d)} \neq \textit{S(P,d)}$}
	\EndProcedure\vspace{-12pt}
	\end{spacing}
	\end{algorithmic}
\end{algorithm}

\subsubsection{Modified Simple Boundary Follower}

% SBF cannot trace an inner-outer corner pixel that is located at the left-rear. MSBF \cite{Gose1996Pattern} was invented to trace these pixels. If the tracer is adjacent to the left-rear inner-outer corner, this condition implies that its left-rear pixel is black (object) and the left and rear pixels are white (background); the tracer will move to the left-rear pixel and its direction is then changed toward the rear direction. After the movement, the tracer goes directly to the left pixel to avoid an infinite loop. Figure \ref{fig:image3} shows examples of the SBF and MSBF paths for a left-rear direction inner-outer corner. In the case of the SBF, if the tracer is on pixel $A$ with direction $N$, it misses pixel $B$. On the contrary, the MSBF can detour pixel $B$.

SBF cannot trace an inner-outer corner pixel that is located at the left rear, and the modified SBF (MSBF)~\cite{Gose1996Pattern} was developed to trace these pixels. If the tracer is adjacent to the left-rear inner-outer corner, this condition implies that its left-rear pixel is black (object), and the left and rear pixels are white (background); the tracer will move to the left-rear pixel, and its direction is then changed toward the rear direction. After the movement, the tracer goes directly to the left pixel to avoid an infinite loop. Figure \ref{fig:image3} shows examples of the SBF and MSBF paths for a left-rear direction inner-outer corner. In the case of the SBF, if the tracer is on pixel $A$ with direction $N$, it misses pixel $B$. On the contrary, the~MSBF can detour pixel $B$.

%%% Image 3
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{2.RelatedWorks/fig3-a.png} \label{fig:img3-a} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{2.RelatedWorks/fig3-b.png} \label{fig:img3-b} }
	 
	\caption{Detour of the inner-outer corner at the left-rear pixel (modified version of \cite{Gose1996Pattern}). (\textbf{a}) Simple boundary follower (SBF); (\textbf{b}) modified SBF (MSBF).}
	\label{fig:image3}
\end{figure}

\subsubsection{Improved Simple Boundary Follower}

% The SBF and MSBF require movement operations for both contour and background pixels; therefore, they waste time during movement on the background pixel and they cannot trace the inner corner pixel in front of the tracer \cite{Cheong2006Improved,Toussaint????Grids}. Hence, the ISBF \cite{Cheong2006Improved} is proposed based on our previous research for overcoming these limitations. The ISBF has six cases for following contour pixels based on the local patterns of the contour pixels. The modified version of \cite{Cheong2012Advanced} is as follows: 

The SBF and MSBF require movement operations for both contour and background pixels; therefore, time is wasted during movement on the background pixel, and they cannot trace the inner-corner pixel in front of the tracer \cite{Cheong2006Improved,Ghuneim2015Contour}. Hence, we have proposed an improved SBF (ISBF)~\cite{Cheong2006Improved} that is based on our previous research aimed at overcoming these limitations. The ISBF has six cases for following contour pixels based on the local patterns of the contour pixels. The modified version of \cite{Cheong2012Advanced} is as \hl{follows}: 


\begin{algorithm}[H]
	\caption{Algorithm of the improved simple boundary follower.}\label{alg:isbf}
	\begin{algorithmic}[1]
	\begin{spacing}{1.5}%%
	\Procedure{ISBF}{}
	\State $\textit{T(P,d)} \gets \textit{S(P,d)}$, where \textit{P} is on black
	\Do
	\If {$\textit{P}_{Left}$ = black}
		\LineComment{Case \ref{fig:img4-a}: Left neighbor}
		\State $\textit{T(P,d)} \gets \textit{T(P}_{Left},\textit{d}_{Left} ) $
	\Else
		\If {$\textit{P}_{Left-Rear}$ = black and $\textit{P}_{Rear}$ = white}
			\LineComment{Case \ref{fig:img4-b}: Inner-outer corner at left-rear}
			\State $\textit{T(P,d)} \gets \textit{T(P}_{Left-Rear},\textit{d}_{Rear} ) $
		\Else
			\If {$\textit{P}_{Front-Left}$ = black}
				\If {$\textit{P}_{Front}$ = black}
					\LineComment{Case \ref{fig:img4-d}: Inner corer at front}
					\State $\textit{T(P,d)} \gets \textit{T(P}_{Front},\textit{d} ) $
					\State $\textit{T(P,d)} \gets \textit{T(P}_{Left}, \textit{d})$
				\Else
					\LineComment{Case \ref{fig:img4-c}: Inner-outer corner at front-left}
					\State $\textit{T(P,d)} \gets \textit{T(P}_{Front-Left},\textit{d} ) $
				\EndIf
			\ElsIf {$\textit{P}_{Front}$ = black}
				\LineComment{Case \ref{fig:img4-e}: Front neighbor}
				\State $\textit{T(P,d)} \gets \textit{T(P}_{Front},\textit{d}_{Right} ) $
			\Else
				\LineComment{Case \ref{fig:img4-f}: Outer corner}
				\State $\textit{T(P,d)} \gets \textit{T(P},\textit{d}_{Rear} ) $
			\EndIf
		\EndIf
	\EndIf \linebreak\vspace{-6pt}
	\doWhile {$ \textit{T(P,d)} \neq \textit{S(P,d)}$}
	\EndProcedure\vspace{-12pt}
	\end{spacing}%%
	\end{algorithmic}
\end{algorithm}

% Figure \ref{fig:image4} represents the tracing path of the ISBF based on the local contour patterns. While the SBF cannot trace in cases \ref{fig:img4-b} and \ref{fig:img4-d}, and the MSBF cannot follow in case \ref{fig:img4-d}, the ISBF successfully traces in all the cases. In the figure, the way point (dotted line) is subjected to a detection operation for determining whether it is black or white without employing a movement operation.

Figure \ref{fig:image4} represents the tracing path of the ISBF based on the local contour patterns. While the SBF cannot trace in Cases \ref{fig:img4-b} and \ref{fig:img4-d} and the MSBF cannot follow in Case \ref{fig:img4-d}, the ISBF successfully traces in all of the cases. In the figure, the waypoint (dotted line) is subjected to a detection operation to determine whether it is black or white without employing a movement operation.

%%% Image 4
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[height=2cm]{2.RelatedWorks/fig4-a.png} \label{fig:img4-a} }
	\subfloat[]{ \includegraphics[height=2cm]{2.RelatedWorks/fig4-b.png} \label{fig:img4-b} }
	\subfloat[]{ \includegraphics[height=2cm]{2.RelatedWorks/fig4-c.png} \label{fig:img4-c} }
	\subfloat[]{ \includegraphics[height=2cm]{2.RelatedWorks/fig4-d.png} \label{fig:img4-d} }
	\subfloat[]{ \includegraphics[height=2cm]{2.RelatedWorks/fig4-e.png} \label{fig:img4-e} }
	\subfloat[]{ \includegraphics[height=2cm]{2.RelatedWorks/fig4-f.png} \label{fig:img4-f} }
	\subfloat{ \includegraphics[height=2cm]{2.RelatedWorks/fig4-g.png} }
	 
	\caption{Contour cases of the improved SBF (ISBF) \cite{Cheong2012Advanced}: (\textbf{a}) left neighbor; (\textbf{b}) inner-outer corner at the left-rear; (\textbf{c}) inner-outer corner at the front-left; (\textbf{d}) inner corner at the front; (\textbf{e}) front neighbor; (\textbf{f}) outer corner.}
	\label{fig:image4}
\end{figure}

\subsubsection{Moore-Neighbor Tracing}
% MNT (see, figure \ref{fig:img5-a}) finds the next contour pixel using 8 connected chain codes using a clockwise sequence starting from the rear pixel of the tracer, i.e., the tracer first moves toward the rear ($T (P_{Rear}, d_{Rear}))$ and finds the next clockwise contour pixel such as the left-rear, left, font-left, front, front-right, right, and right-rear pixels \cite{Toussaint????Grids}. 
Moore-neighbor tracing (MNT) finds the next contour pixel using eight connected chain codes with a clockwise sequence starting from the rear pixel of the tracer, \emph{i.e.}, the tracer first moves toward the rear ($T (P_{Rear}, d_{Rear})$) and finds the next clockwise contour pixel, such as the left-rear, left, font-left, front, front-right, right and rear-right pixels \cite{Toussaint????Grids, Ghuneim2015Contour}. 

%%%%%%%%%%%%%%%%%%%%%% TO BE WRITTEN!!!!
%%%%%%%%%%%%%%%%%%%%%% TO BE WRITTEN!!!!
%%%%%%%%%%%%%%%%%%%%%% TO BE WRITTEN!!!!
% \subsubsection{Modified Moore-Neighbor Tracing}
% \JHMEMO{To be written}
%%%%%%%%%%%%%%%%%%%%%% TO BE WRITTEN!!!!
%%%%%%%%%%%%%%%%%%%%%% TO BE WRITTEN!!!!
%%%%%%%%%%%%%%%%%%%%%% TO BE WRITTEN!!!!

\subsubsection{Radial Sweep Algorithm}
% RSA \cite{Mirante1982Radial} is similar to MNT, but its tracer has no directional information. Therefore, it maintains two points, namely, previous pixel and current pixel for the initial tracing direction. Figure \ref{fig:img5-b} illustrates an example of a tracing path by using RSA from $P_i$ to $P_{i+2}$. In the figure, the direction vector from $P_i$ to $P_{i-1}$ is first generated, and then the tracer searches for the next contour pixel using the previous pixel $P_{i-1}$ for the clockwise direction of the vector.

The radial sweep algorithm (RSA) \cite{Mirante1982Radial} is similar to MNT, but its tracer has no directional information. Therefore, it maintains two points, namely the previous pixel and current pixel for the initial tracing direction. Figure \ref{fig:img5-b} illustrates an example of a tracing path obtained using RSA from $P_i$ to $P_{i+2}$. In the figure, the direction vector from $P_i$ to $P_{i-1}$ is first generated, and the tracer then searches for the next contour pixel using the previous pixel $P_{i-1}$ for the clockwise direction of the~vector.

%%% Image 5
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{2.RelatedWorks/fig5-a.png} \label{fig:img5-a} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{2.RelatedWorks/fig5-b.png} \label{fig:img5-b} }
	\caption{Contour-following sequence of Moore-neighbor tracing (MNT) and the radial sweep algorithm (RSA): (\textbf{a}) \hl{MNT} \cite{Toussaint????Grids}; (\textbf{b}) RSA~\cite{Mirante1982Radial}.}
	\label{fig:mnt_rsa}
\end{figure}


\subsubsection{Theo Pavlidis Algorithm}
% TPA \cite{Pavlidis2012Algorithms} considers only three adjacent pixels, e.g., front-left, front, and front-right for determining the next contour pixel. If all the three pixels are white, the tracer turns right. Figure \ref{fig:image6} describes its sequence. 
To determine the next contour pixel, the Theo Pavlidis algorithm (TPA) \cite{Pavlidis2012Algorithms} considers only three adjacent pixels, e.g., front-left, front and front-right. If all three pixels are white, the tracer turns right. Figure \ref{fig:image6} describes the sequence of this algorithm. 

%%% Image 6
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.2\textwidth]{2.RelatedWorks/fig6-a.png} \label{fig:img6-a} }
	\subfloat[]{ \includegraphics[width=0.2\textwidth]{2.RelatedWorks/fig6-b.png} \label{fig:img6-b} }
	\subfloat[]{ \includegraphics[width=0.2\textwidth]{2.RelatedWorks/fig6-c.png} \label{fig:img6-c} }
	\subfloat[]{ \includegraphics[width=0.2\textwidth]{2.RelatedWorks/fig6-d.png} \label{fig:img6-d} }
	 
	\caption{Contour-following sequence of the Theo Pavlidis algorithm (TPA)~\cite{Cheong2012Advanced}. (\textbf{a}) Front-left contour; (\textbf{b}) front contour; (\textbf{c}) front-right contour; (\textbf{d}) rotation.}
	\label{fig:image6}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conventional Data Compression and Restoration}
% The RD code method \cite{Miyatake1997Contour} comprises two major techniques. One traces the contour on the basis of the hybrid method using vertex following with run data and generates the corresponding RD codes. The other generates compressed contour data that can be used to restore the contour on the basis of representative points and their corresponding RD codes. The representative points are selected from the vertices of contour pixels and they are feature points of the contour. Moreover, the RD code can represent 10 local contour patterns and their corresponding following paths. Therefore, saving the representative points and their corresponding RD codes rather than all the contour points can reduce the memory size used to store the contour data. Figure \ref{fig:img7-a} \cite{Miyatake1997Contour} gives an example of representative points. The representative points in the RD method are of four types, namely, two outer corner points and two inner corner points, as shown in figure \ref{fig:img7-b}. Although this technique can save data in small files; it does not consider the inner-outer corner pixel. 

The RD code method \cite{Miyatake1997Contour} comprises two major techniques. The first one traces the contour using a hybrid method that employs vertex following with run data, and it generates the corresponding RD codes. The other generates compressed contour data that can be used to restore the contour based on representative points and their corresponding RD codes. The representative points are selected from the vertices of contour pixels, and they are feature points of the contour. Moreover, the RD code can represent 10 local contour patterns and their corresponding following paths. Therefore, by saving the representative points and their corresponding RD codes instead of all of the contour points, we can reduce the memory size used to store the contour data. Figure \ref{fig:img7-a} \cite{Miyatake1997Contour} gives an example of representative points. There are four types of representative points in the RD method, namely two outer corner points and two inner corner points, as shown in Figure \ref{fig:img7-b}. Although this technique can save data in small files, it does not consider the inner-outer corner pixel. 

%%% Image 7
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[height=4.5cm]{2.RelatedWorks/fig7-a.png} \label{fig:img7-a} }
	\subfloat[]{ \includegraphics[height=4.5cm]{2.RelatedWorks/fig7-b.png} \label{fig:img7-b} }
	\caption{Representative points for data compression. (\textbf{a}) Representative points \cite{Miyatake1997Contour}; (\textbf{b}) cases of representative points.}
	\label{fig:rdcode}
\end{figure}
%-----------------------------------------------------------------------------------------



%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

\section{Analysis of Conventional Contour-Following Algorithms}

% Figure \ref{fig:image8} illustrates the tracing results of pixel following methods based on the contour shown in figure \ref{fig:image1}. In the figure, the arrow with an anchor is the tracer at the starting pixel, a solid arrow shows the movement operation of the tracer, and a dotted line is the way point (detected pixel) for determining whether or not the pixel is a contour pixel for pixel following.

Figure \ref{fig:image8} illustrates the tracing results of the pixel-following methods based on the contour shown in Figure \ref{fig:image1}. In the figure, the arrow with an anchor is the tracer at the starting pixel, the solid arrow shows the movement operation of the tracer and the dotted line is the way point (detected pixel) for determining whether or not the pixel is a contour pixel for pixel following.

%%% Image 8
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{3.Analysis/fig8-a.png} \label{fig:img8-a} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{3.Analysis/fig8-b.png} \label{fig:img8-b} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{3.Analysis/fig8-c.png} \label{fig:img8-c} } \\
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{3.Analysis/fig8-d.png} \label{fig:img8-d} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{3.Analysis/fig8-e.png} \label{fig:img8-e} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{3.Analysis/fig8-f.png} \label{fig:img8-f} }
	 
	\caption{Comparison of conventional contour pixel-following algorithms: (\textbf{a}) SBF; (\textbf{b}) MSBF; (\textbf{c}) ISBF; (\textbf{d}) MNT; (\textbf{e}) RSA; (\textbf{f})~TPA.}
	\label{fig:image8}
\end{figure}

%%%%%
\subsection{Pixel-Following Cases}

% As shown in figure \ref{fig:image8}, the ISBF traces most types of contour pixels such as the inner corner, outer corner, inner-outer corner, and straight-line pixels. In the case of the SBF, there are inconsistencies regarding tracing at the inner corner and inner-outer corner. For example, in the figure, the inner corner pixel (3, 6) and inner-outer corner pixel (7, 8) are traced, but the inner corner (3, 4) and inner-outer corner (7, 9) pixels are missed. In the case of the MSBF, all the inner-outer corner pixels are traced, but it has inconsistencies with regard to tracing the inner corner pixel. Moreover, the MNT, RSA, and TPA have no inconsistency problems but they cannot trace the inner corners. Among these algorithms, the TPA can be easily changed to trace an inner corner pixel since it has way points on the inner corners, as shown in figure \ref{fig:img8-f}. 

As shown in Figure \ref{fig:image8}, ISBF traces most types of contour pixels, such as the inner corner, outer corner, inner-outer corner and straight-line pixels. In the case of the SBF, there are inconsistencies regarding tracing at the inner corner and inner-outer corner. For example, in the figure, the inner-corner pixel $(3, 6)$ and inner-outer corner pixel $(7, 8)$ are traced, but the inner corner $(3, 4)$ and inner-outer corner $(7, 9)$ pixels are missed. In the case of the MSBF, all of the inner-outer corner pixels are traced, but it has inconsistencies with regard to tracing the inner-corner pixel. Moreover, MNT, RSA and TPA have no problems with consistency, but they cannot trace the inner corners. Among these algorithms, the TPA can be easily changed to trace an inner-corner pixel, because it has waypoints on the inner corners, as shown in Figure \ref{fig:img8-f}.



%%%%%
\subsection{Start-Up Condition and Stopping Criteria}

% The pixel following algorithm functions under the criteria of start and stop to avoid incompleteness of tracing and infinite tracing.
The pixel-following algorithm requires start and stop criteria to avoid incomplete or infinite contour tracing.

\subsubsection{Assumption for Start}

% Commonly, start of tracing occurs when the tracer enters a black pixel from a white pixel. Due to this reason, at the start of tracing, the tracer must be placed on a black pixel and its rear pixel $P_{Rear}$ should be white. Moreover, in the case of the MSBF and ISBF, if the $P_{Left-Rear}$ of the start pixel is an inner-outer corner pixel, they cannot trace all the contour pixels by using their stopping criteria \cite{Cheong2012Advanced}. In addition, TPA has to select a start pixel that has white pixels at the tracer positions $P_{Left}$, $P_{Left-Rear}$, and $P_{Right-Rear}$\cite{Ghuneim2015Contour}; otherwise, it cannot trace the left pixel of the contour. 

Commonly, tracing starts when the tracer enters a black pixel from a white pixel. Therefore, at~the start of tracing, the tracer must be placed on a black pixel, and its rear pixel $P_{Rear}$ should be white. Moreover, in the case of the MSBF and ISBF, if the $P_{Left-Rear}$ of the start pixel is an inner-outer corner pixel, it cannot trace all of the contour pixels using their stopping criteria \cite{Reddy2012Evaluation,Cheong2012Advanced}. In addition, TPA has to select a start pixel that has white pixels at the tracer positions $P_{Left}$, $P_{Left-Rear}$ and $P_{Right-Rear}$ \cite{Ghuneim2015Contour}; otherwise, it cannot trace the left pixel of the contour. 

\subsubsection{Stop Criterion}
% There are three methods for stopping the contour tracing \cite{Ghuneim2015Contour}. The first method is Jacob's stopping criterion \cite{Ghuneim2015Contour} that terminates the trace when the tracer reenters the start pixel with an absolute direction that is the same as the start direction, i.e., if the current tracer $T(P,d)$ is the same as the start tracer $S(P,d)$, the pixel following is terminated. The SBF, MSBF, and ISBF use this criterion and their tracing terminates at the start pixel, as shown in figures \ref{fig:img8-a}-\ref{fig:img8-c}. The second method uses the number of reentries to the start pixel. In figures \ref{fig:img8-d} and \ref{fig:img8-f}, the tracers of MNT and TPA revisit the start pixel (1, 5) with different directional information; therefore, they do not stop but go to the next contour pixel if the first method is applied. For this reason, if a specified number of reentries such as three or four times is satisfied, the trace is terminated \cite{Ghuneim2015Contour}. Sometimes, this method is not efficient because it requires unnecessary tracing to be performed one or more times. The final method checks the trace route that is traced by the previous pixel and current pixel of the tracer and determines whether or not it has already been passed. This method is used for RSA \cite{Ghuneim2015Contour,Mirante1982Radial} since its tracer has no directional information but only pixel location information. In other words, whenever the tracer enters the $i$-th contour pixel, the current pixel location $P_i$ is appended sequentially into the traced contour path. Moreover, if the traced path of $(P_{i-1}, P_i)$ appears twice, the tracing is terminated. This method can be applied for all pixel following methods and it is simpler than the second method; however, it requires more operations than Jacob's stopping criterion.

There are three methods for stopping the contour tracing \cite{Ghuneim2015Contour,Reddy2012Evaluation}. The first method is Jacob's stopping criterion \cite{Ghuneim2015Contour}, which terminates the trace when the tracer reenters the start pixel with an absolute direction that is the same as the start direction, \emph{i.e.}, if the current tracer $T(P,d)$ is the same as the start tracer $S(P,d)$, the pixel following is terminated. SBF, MSBF and ISBF use this criterion, and their tracing terminates at the start pixel, as shown in Figure \ref{fig:image8}a--c. The second method uses the number of reentries to the start pixel. In Figure \ref{fig:image8}d--f, the tracers of MNT and TPA revisit the start pixel $(1, 5)$ with different directional information; therefore, they do not stop, but rather go to the next contour pixel if the first method is applied. For this reason, if a specified number of reentries, e.g., three or four times, is satisfied, the trace is terminated \cite{Reddy2012Evaluation}. This method is sometimes not efficient, because it requires unnecessary tracing to be performed one or more times. The final method checks the trace route that is traced by the previous pixel and the current pixel of the tracer and determines whether it has already been passed. This method is used for RSA \cite{Ghuneim2015Contour,Mirante1982Radial} because its tracer has only pixel-location information and no directional information. In other words, whenever the tracer enters the $i$-th contour pixel, the current pixel location $P_i$ is appended sequentially into the traced contour path. Moreover, if the traced path of $(P_{i-1}, P_i)$ appears twice, the tracing is terminated. This method can be applied for all pixel-following methods, and it is simpler than the second method; however, it requires more operations than Jacob's stopping criterion.

\subsubsection{Limitations of Conventional Pixel-Following Methods}

% The abovementioned conventional pixel following methods have certain limitations. First, some of the algorithms such as SBF and MSBF perform unnecessary movement operation on a white pixel, as shown in figures \ref{fig:img8-a} and \ref{fig:img8-b}. Second, all the algorithms cannot define the contour in the case of contour pixels; therefore, they cannot be a descriptive feature of the object and determine connectivity among objects. For example, in figure \ref{fig:img8-b}, the MSBF detects (2, 7) as the inner-outer corner pixel, but does not indicate (8, 7) as an inner-outer corner because the traced paths on the pixels are different. Moreover, the MSBF also cannot determine the inner corner, outer corner, and straight line pixel like the SBF does. In case of the ISBF, it determines the inner-outer corner, front inner corner, and front straight line pixels but cannot determine the left inner corner, left straight line and some of the outer corner pixels, as shown in figure \ref{fig:img8-c}. Similarly, MNT and RSA cannot determine and detect the inner corners, and TPA cannot classify the contour pixels into the different types of contour pixels. Finally, the data size of the traced contour must be considered. The pixel following methods save all the pixel points; therefore, their data are larger than those of the RD code method.

The above-mentioned conventional pixel-following methods have certain limitations. First, some of the algorithms, such as SBF and MSBF, perform unnecessary movement operations on white pixels, as shown in Figure \ref{fig:image8}a,b. Second, not all of the algorithms can define the contour in the case of contour pixels; therefore, they cannot be a descriptive feature of the object and determine connectivity among objects. For example, in Figure \ref{fig:img8-b}, MSBF detects $(2, 7)$ as the inner-outer corner pixel, but does not indicate $(8, 7)$ as an inner-outer corner, because the traced paths on the pixels are different. Moreover, MSBF also cannot determine the inner corner, outer corner and straight-line pixel, as is the case with SBF. In the case of ISBF, it determines the inner-outer corner, front-inner corner and front-straight line pixels, but cannot determine the left-inner corner, left-straight line and some of the outer-corner pixels, as shown in Figure \ref{fig:img8-c}. Similarly, MNT and RSA cannot determine and detect the inner corners, and TPA cannot classify the contour pixels into the different types of contour pixels. Finally, the data size of the traced contour must be considered. The pixel-following methods save all of the pixel points; therefore, their data are larger than those of the RD code method.



%-----------------------------------------------------------------------------------------



%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

\section{Proposed Contour-Tracing Algorithm}

% In this section, we propose a novel pixel following algorithm that traces contour pixels by considering their local patterns. Therefore, it can classify the contour pixel into contour types such as inner corner, outer corner, inner-outer corner, and straight line; further, it can easily determine the next contour pixel. Moreover, it can determine and save representative points of an image, like the RD code method, by using pixel following but not using run-data-based following. In addition, the data can be restored to the original contour pixels without the RD code data.

In this section, we propose a novel pixel-following algorithm that traces contour pixels by considering their local patterns. Therefore, it can classify the contour pixel as inner corner, outer corner, inner-outer corner and straight-line contour types. Further, it can easily determine the next contour pixel. Moreover, it can determine and save representative points of an image, such as the RD code method, by using pixel following, but not using run-data-based following. In addition, the data can be restored to the original contour pixels without the RD code data.



\newpage
\subsection{Contour Following}
\unskip
\subsubsection{Assumptions for Start-Up and Stopping Criteria}

% The proposed algorithm runs under two assumptions for start. One is the general condition for pixel following: the rear pixel of the tracer on the start pixel is white. The other is that there is no left-rear inner-outer corner for the tracer at the start position, i.e., if the rear and the left pixels are white and the rear-left pixel is black, the start pixel has to be changed. These are the same starting conditions as those for the MSBF and ISBF. Moreover, the stop criteria of the proposed algorithm is Jacob's stopping criterion and the tracer is always on a contour pixel.

The proposed algorithm runs under two assumptions for starting. One is the general condition for pixel following, where the rear pixel of the tracer on the start pixel is white. The other is that there is no left-rear inner-outer corner for the tracer at the start position, \emph{i.e.}, if the rear and the left pixels are white and the rear-left pixel is black, the start pixel has to be changed. These are the same starting conditions as those used for MSBF and ISBF. Moreover, the stop criteria of the proposed algorithm is Jacob's stopping criterion, and the tracer is always on a contour pixel. 

% Image 9



\subsubsection{Procedures}

% The proposed algorithm has two stages. First, the tracer follows the contour pixel on the basis of the intensities of the left-rear and left pixels. After that, the tracer follows the contour pixels on the basis of the intensities of the front and front-left pixels. Figure \ref{fig:image9} shows the contour tracing cases of the proposed contour following algorithm. In the figure, the tracer is first on $N0$ queries states of $N1$ and $N2$, as shown in figure \ref{fig:image9} (a), and then the states determine the corresponding path to trace among cases (1)-(4), as shown in figure \ref{fig:image9} (b). After stage 1, the moved tracer queries states N3 and N4 and then it traces contour pixels along the corresponding path using the states among cases (5)-(8). 

The proposed algorithm has two stages. First, the tracer follows the contour pixel based on the intensities of the left-rear and left pixels. After that, the tracer follows the contour pixels according to the intensities of the front and front-left pixels. Figure \ref{fig:image9} shows the contour tracing cases for the proposed contour-following algorithm. In the figure, the tracer is first on $N0$ and queries states of $N1$ and $N2$, as shown in Figure \ref{fig:img9-a}, and then, the states determine the corresponding path to be traced from among Cases (1)--(4), as shown in Figure \ref{fig:img9-b}. After Stage 1, the moved tracer queries states $N3$ and $N4$, and it then traces contour pixels along the corresponding path using the states from among Cases~(5)--(8). 


\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.2\textwidth]{4.Proposed/fig9-a.png} \label{fig:img9-a} }
	\subfloat[]{ \includegraphics[width=0.7\textwidth]{4.Proposed/fig9-b.png} \label{fig:img9-b} }

	\caption{Contour tracing cases for the proposed contour-following algorithm. (\textbf{a}) Adjacent pixels; (\textbf{b})~contour cases.}
	\label{fig:image9}
\end{figure}

% Hence, by using the proposed algorithm, the inner corners are traced as case (1) or case (6), the inner-outer corners are traced as case (2) or case (5), the outer corners are determined as case (4) or case (8), and the straight-line pixels are determined as the other cases. Therefore, all the cases are easily classified by using the algorithm. 

Hence, by using the proposed algorithm, the inner corners traced are considered as Case (1) or (6); the inner-outer corners are considered as Case (2) or (5); the outer corners are considered as Case (4) or (8); and the straight-line pixels are considered as the other cases. Therefore, all of the cases are easily classified using the algorithm. 




\subsubsection{States}

% Image 10


% Figure \ref{fig:image10} describes the state transition for the automation of the proposed algorithm. In the figure, the start and termination occur at State 0; the first stage runs using States 1-4 and then it transits to State 5. The second stage continues from State 5 using States 6-11 and then it transits to State 0. For example, case (1) in figure \ref{fig:image9} can be executed by the transit sequence State 1, State 3, and State 5. 

Figure \ref{fig:image10} describes the state transition for the automation of the proposed algorithm. In the figure, the start and termination occur at State 0. The first stage runs using States 1--4, and it then transits to State 5. The second stage continues from State 5 using States 6--11, and it then transits to State 0. For~example, Case (1) in Figure \ref{fig:img9-b} can be executed using the transit sequence State 1, State 3 and State~5.

% In figure \ref{fig:image10}, $[N1 N2 N3 N4]$ represents the intensity vector of the tracer's four adjacent pixels shown in figure \ref{fig:image9} (a). In the vector, $d$ implies ``do not care''; $B$, a contour pixel (black); and $W$, the background pixel (white). Moreover, Update $(P,d)$ refers to the movement operation where $P$ is the new contour pixel location and $d$ is the new directional information for the tracer. In State 1 and State 3, the updates occur based on the tracer of State 0, and other updates are based on the tracer information in State 5.

\newpage
In Figure \ref{fig:image10}, $[N1\ N2\ N3\ N4]$ represents the intensity vector of the tracer's four adjacent pixels shown in Figure \ref{fig:img9-a}. In the vector, $d$ implies ``do not care''; $B$ represents a contour pixel (black) and $W$ represents the background pixel (white). Moreover, update $(P,d)$ refers to the movement operation, where $P$ is the new contour-pixel location and $d$ is the new directional information for the tracer. In~States 1 and 3, the updates occur based on the tracer of State 0, and other updates are based on the tracer information in State 5.


\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{4.Proposed/state.png}
	\caption{State transition of automation for the proposed algorithm.}
	\label{fig:image10}
\end{figure}

\subsubsection{Characteristics of the Proposed Algorithm}

% We designed the proposed algorithm based on two stages with two major goals. First, the check operation for stopping occurs only at State 0; therefore, the number of checking operations on black pixels are reduced. In other words, the proposed algorithm examines that the check operation occurs when only the tracer has a white rear pixel. This is more efficient as compared to the check operation occuring for every contour pixel because its start condition and stop criterion also satisfy that the rear pixel of the tracer on the start pixel is white. Therefore, the transition of stage 2 to States 6 to 11 for processing cases (5) to (8) is designed such that it can be returned to State 0 only if the tracer has a white rear pixel, and it reduces any unnecessary operations for checking to stop. Besides, the tracers of cases (2) and (4) have a white rear pixel after movement but their end conditions are not considered. In case (2), at the start the tracer avoids the inner-outer corner pixels as the start and end pixel. Moreover, in case (4), the tracer has no update; therefore, it is unnecessary to perform check operation twice. 

We designed the proposed algorithm based on two stages with two major goals. First, the check operation for stopping occurs only at State 0; therefore, the number of checking operations on black pixels is reduced. In other words, the proposed algorithm verifies that the check operation occurs when only the tracer has a white rear pixel. This is more efficient when compared to the check operation that occurs for every contour pixel, because its start condition and stop criterion also satisfy the condition that the rear pixel of the tracer on the start pixel should be white. Therefore, the transition of \linebreak Stage 2 to States 6--11 for processing Cases (5)--(8) is designed such that it can be returned to State 0 only if the tracer has a white rear pixel, and it reduces any unnecessary operations that are required to stop the checking. Besides, the tracers of Cases (2) and (4) have a white rear pixel after the movement, but their end conditions are not considered. In Case (2), at the start, the tracer avoids the inner-outer corner pixels as the start and end pixel. Moreover, in Case (4), the tracer has no update; therefore, it is unnecessary to perform the check operation twice. 

% Second, the proposed algorithm reduces some of the redundant operations used for detecting white pixels. The conventional algorithms do not consider white pixels in the previous path; therefore, they sometimes re-detect white pixels on the previous tracing during the current tracing. For example, in figure 8, the white pixel at (4, 2) is detected twice while determining contour pixels such as (4, 3) and (5, 3). Moreover, RSA detects (4, 2) three time while determining (4, 3), (5, 3), and (6, 3). On the contrary, the proposed algorithm has two stages and its second stage avoids the previous path. \textcolor{red}{Figure 11} shows the contour tracing results for the proposed algorithm and it detects (4, 2) once for contour tracing. Moreover, the figure shows that the proposed algorithm has fewer operations on the white pixels as compared to conventional pixel following methods, as shown in figure \ref{fig:image8}, and traces all the types of contours. 

Second, the proposed algorithm eliminates some of the redundant operations that are used to detect white pixels. The conventional algorithms do not consider white pixels in the previous path; therefore, they sometimes re-detect white pixels on the previous tracing during the current tracing. For~example, in Figure \ref{fig:image8}, the white pixel at $(4, 2)$ is detected twice while determining contour pixels, such as $(4, 3)$ and $(5, 3)$. Moreover, RSA detects $(4, 2)$ three times while determining $(4, 3)$, $(5, 3)$ and $(6, 3)$. On the contrary, the proposed algorithm has two stages, and its second stage avoids the previous path. Figure \ref{fig:image11} shows the contour-tracing results obtained for the proposed algorithm, and it detects $(4, 2)$ once for contour tracing. Moreover, the figure shows that the proposed algorithm has fewer operations on the white pixels when compared to conventional pixel-following methods, as shown in Figure \ref{fig:image8}, and it traces all types of contours. 

% Image 11
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{4.Proposed/fig11.png}
	\caption{Result of contour tracing using the proposed algorithm.}
	\label{fig:image11}
\end{figure}

% The pseudo-code of the proposed algorithm is as below: 

The pseudocode of the proposed algorithm is shown in Algorithm \ref{alg:proposed}.



% Table \ref{table:table2} describes the tracing results obtained by using the proposed algorithm up to the stage the tracer enters the 11th pixel. The code represents the contour pixel type and it is classified automatically during tracing. 

Table \ref{table:table2} describes the tracing results that were obtained by using the proposed algorithm up to the stage at which the tracer enters the 11th pixel. The code represents the contour pixel type, and it is classified automatically during tracing. 

\begin{table}[H]
	\centering
	\begin{tabular}{cccc}
		\toprule
		\multirow{2}{*}{\boldmath{$Sequence(i)$}} & \multicolumn{2}{c}{\textbf{P}} & \multirow{2}{*}{\boldmath{$Code(i)$}} \\
		 \cmidrule(rl){2-3}
		      &\boldmath{$x$}   & \boldmath{$y$} & \\
		\midrule
		1 & 1 & 5 & Outer \\
		2 & 2 & 5 & Inner \\
		3 & 2 & 4 & Outer \\
		4 & 3 & 4 & Inner \\
		5 & 3 & 3 & Outer \\
		6 & 4 & 3 & Straight \\
		7 & 5 & 3 & Straight \\
		8 & 6 & 3 & Inner-outer \\
		9 & 7 & 2 & Inner-outer \\
		10 & 7 & 1 & Outer \\
		11 & 8 & 1 & \\

		\bottomrule
	\end{tabular}
	\caption{Result table of the proposed contour tracing.}
	\label{table:table2}
\end{table}


\begin{algorithm}[H]
	\caption{Procedure of the proposed tracer.}
	\label{alg:proposed}
	\begin{algorithmic}[1]
	\begin{spacing}{1.5}%%
	\Procedure{Proposed\_Tracer}{}
	\State $\textit{T(P,d)} \gets \textit{S(P,d)}$, where $P$ is on black, $P_{Rear}$ is on white and $i \gets 1$
	\LineComment{Whenever $T(P,d)$ is updated, $i$ increases by%please check the correction
	$1$ and $T(p',d')$ is saved automatically}
	\Do
	\LineComment{Stage 1}
	\If {$\textit{P}_{Left-Rear}$ = black}
		\If {$\textit{P}_{Left}$ = black}
			\LineComment{Case 1}
			\State $\textit{T(P,d)} \gets \textit{T(P}_{Left},\textit{d}_{Left} ) $ and $\textit{Code(i)} \gets ``Inner''$
			\State $\textit{T(P,d)} \gets \textit{T(P}_{Left}, \textit{d}_{Left})$
		\Else
			\LineComment{Case 2}
			\State $\textit{Code(i)} \gets ``Inner-outer''$
			\State $\textit{T(P,d)} \gets \textit{T(P}_{Left-Rear},\textit{d}_{Rear} ) $ and $\textit{Code(i)} \gets ``Inner-outer''$
		\EndIf
	\Else
		\If {$\textit{P}_{Left}$ = black}
			\LineComment{Case 3}
			\State $\textit{T(P,d)} \gets \textit{T(P}_{Left},\textit{d}_{Left} ) $ and $\textit{Code(i)} \gets ``Straight''$
		\Else
			\LineComment{Case 4}
			\State $\textit{Code(i)} \gets ``Outer''$ \vspace{6pt}
		\EndIf
	\EndIf


	\LineComment{Stage 2}
	\If {$\textit{P}_{Front-Left}$ = black}
		\If {$\textit{P}_{Front}$ = black}
			\LineComment{Case 6}
			\State $\textit{T(P,d)} \gets \textit{T(P}_{Front},\textit{d}_{Left} ) $ and $\textit{Code(i)} \gets ``Inner''$
			\State $\textit{T(P,d)} \gets \textit{T(P}_{Front}, \textit{d}_{Right})$
		\Else
			\LineComment{Case 5}
			\State $\textit{Code(i)} \gets ``Inner-outer''$
			\State $\textit{T(P,d)} \gets \textit{T(P}_{Front-Left},\textit{d} ) $ and $\textit{Code(i)} \gets ``Inner-outer''$
		\EndIf
	\ElsIf {$\textit{P}_{Front}$ = black}
		\LineComment{Case 7}
		\State $\textit{T(P,d)} \gets \textit{T(P}_{Front},\textit{d}_{Right} ) $
	\Else
		\LineComment{Case 8}
		\State $\textit{T(P,d)} \gets \textit{T(P},\textit{d}_{Rear} ) $ and $i \gets i-1$ and $\textit{Code(i)} \gets ``Outer''$
	\EndIf


	\doWhile {$ \textit{T(P,d)} \neq \textit{S(P,d)}$}
	\EndProcedure
	\end{spacing} \vspace{-6pt}
	\end{algorithmic}
\end{algorithm}

% In the table, $Code (i)$ represents only one code, the contour pixel type, per contour pixel but it can have several codes, for example, there is an outer corner pixel and an inner-outer corner pixel. 

In the table, $Code (i)$ represents only one code, the contour pixel type per contour pixel, but it can have several codes. For example, there is an outer-corner pixel and an inner-outer corner pixel. 


%%%%%%%%%%%%%%%%%
 \subsection{Data Compression and Restoration}

 % The proposed algorithm saves representation points and the inner-outer corner points in the form of compressed data in order to reduce the data size. The representation points are feature points that are used for storing and restoring contour pixels, while the inner-outer corner points are used for accurately restoring the inner-outer corner pixels. 

 The proposed algorithm saves representation points and the inner-outer corner points in the form of compressed data in order to reduce the data size. The representation points are feature points that are used for storing and restoring contour pixels, while the inner-outer corner points are used for accurately restoring the inner-outer corner pixels. 


\newpage
 \subsubsection{Data Structure}
 % The representative points and inner-outer corner points are represented as vertices of contour pixels. Figure \textcolor{red}{12 (a)} shows the point types of the proposed algorithm. The representative point is of seven types, namely, two outer corners, two inner corners, and one inner-outer corner. Moreover, the inner-outer corner point has two types. Figure \textcolor{red}{12 (b)} illustrates all cases of contour tracing for the proposed algorithm and their corresponding representative points and inner-outer corner points. 

The representative points and inner-outer corner points are represented as vertices of contour pixels. Figure \ref{fig:img12-a} shows the point types of the proposed algorithm. There are seven types of representative points. They are two outer corners, two inner corners and one inner-outer corner. In addition, there are two types of inner-outer corner point. Figure \ref{fig:img12-b} illustrates all cases of contour tracing for the proposed algorithm and their corresponding representative points and inner-outer corner points. 

 % These points are saved as a sequence during contour tracing. If an $i$-th representative point $R_i$ is equivalent to $(r_{i,x}, r_{i,y})$, then the set of representative points $R$ is given by $\{R_0, R_1, R_2, … , R_{n-1}\}$ and $R_0 = R_n$ because the starting and ending points are the same. Similarly, if $C_j$ is the $j$-th inner-outer corner point, it can be represented using its coordinate and its type. The type of the point $C_{j,T}$ is assigned to be NW-SE or NE-SW, as shown in figures \textcolor{red}{12 (a)}. Table \ref{table:data_structure} shows the data structure for data compression and restoration of the contour pixels by using the proposed algorithm. 

These points are saved as a sequence during contour tracing. If the $i$-th representative point $R_i$ is equivalent to $(r_{i,x}, r_{i,y})$, then the set of representative points $R$ is given by $\{R_0, R_1, R_2, \cdots , R_{n-1}\}$ and $R_0 = R_n$ because the starting and ending points are the same. Similarly, if $C_j$ is the $j$-th inner-outer corner point, it can be represented using its coordinate and its type. The type of point $C_{j,T}$ is assigned to be $NW-SE$ or $NE-SW$, as shown in Figure \ref{fig:img12-a}. Table \ref{table:data_structure} shows the data structure for data compression and the restoration of the contour pixels using the proposed algorithm.

 %%% Image 12
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=1.0\textwidth]{4.Proposed/fig12-a.png} \label{fig:img12-a} } \\
	\subfloat[]{ \includegraphics[width=1.0\textwidth]{4.Proposed/fig12-b.png} \label{fig:img12-b} }
	\caption{Contour pixel reconstruction. (\textbf{a}) Representative points and inner-outer corner points; (\textbf{b})~cases of the proposed algorithm.}
	\label{fig:image12}
\end{figure}
\unskip

 \begin{table}[H]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		\multicolumn{2}{c}{\textbf{Representative Points (\boldmath{$R_i$})}} & 
		\multicolumn{3}{c}{\textbf{Inner-Outer Corner (\boldmath{$C_i$})}} \\
\cmidrule(rl){1-2}\cmidrule(rl){3-5}
		\boldmath{$x$} & \boldmath{$y$} & \boldmath{$x$} & \boldmath{$y$} & \textbf{Type} \\
		\midrule

		$r_{1,x}$ & $r_{1,y}$ & $C_{1,x}$ & $C_{1,y}$ & $C_{1,T}$ \\
		$r_{2,x}$ & $r_{2,y}$ & $C_{2,x}$ & $C_{2,y}$ & $C_{2,T}$ \\
		... & ... & ... & ... & ...\\
		\bottomrule
	\end{tabular}
	\caption{Data structure of the proposed contour tracer.}
	\label{table:data_structure}
\end{table}

\subsubsection{Contour Pixel Restoration}

% For restoration, we proposed a restoration algorithm comprising two stages, namely, contour restoration and inner-outer corner restoration.

For the restoration, we propose a restoration algorithm comprising two stages, namely contour restoration and inner-outer corner restoration.


% \subsubsection{Contour Restoration with Representation Points}
\paragraph{Contour Restoration with Representation Points}

% The sequence of representative points is important information for reconstructing the contour pixels because it represents the contour tracing sequence. Hence, by using the sequence of representative points and the relative location between adjacent representative points in the representative point table, the contour can be restored easily. If there are two sequential representative points $R_i$ and $R_{i+1}$, $\Lambda(R_i, R_{i+1})$ as the relative position classifier from $R_i$ to $R_{i+1}$ can be described as 

The sequence of representative points is important for the reconstruction of the contour pixels, because it represents the contour-tracing sequence. Hence, by using the sequence of representative points and the relative location between adjacent representative points in the representative point table, the contour can be restored easily. If there are two sequential representative points $R_i$ and $R_{i+1}$, $\Lambda(R_i, R_{i+1})$, which is the relative position classifier from $R_i$ to $R_{i+1}$, can be described as: 

\begin{equation}
	\Lambda(R_i, R_{i+1}) = \begin{cases}
	NE\: \text{where\:} r_{i,x} < r_{i+1,x}\: \text{and\:} r_{i,y} > r_{i+1,y} \\ 
	SE\: \text{where\:} r_{i,x} < r_{i+1,x}\: \text{and\:} r_{i,y} < r_{i+1,y} \\ 
	NW\: \text{where\:} r_{i,x} > r_{i+1,x}\: \text{and\:} r_{i,y} > r_{i+1,y} \\ 
	SW\: \text{where\:} r_{i,x} > r_{i+1,x}\: \text{and\:} r_{i,y} < r_{i+1,y}
	\end{cases}
	\label{eq:lambda}
\end{equation}

% Figure \textcolor{red}{13} shows the contour pixel reconstruction methods using the relative positions from $R_i$ to $R_{i+1}$. In the case of SE or NW, the contour pixels are filled in a clockwise manner and in the other cases, these pixels are filled in a counterclockwise manner.
Figure \ref{fig:image13} shows the contour-pixel reconstruction methods using the relative positions from $R_i$ to $R_{i+1}$. In the case of $SE$ or $NW$, the contour pixels are filled in a clockwise manner, while in the other cases, these pixels are filled in a counterclockwise manner.

%%% Image 12
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig13-a.png} \label{fig:img13-a} }
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig13-b.png} \label{fig:img13-b} }
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig13-c.png} \label{fig:img13-c} } 
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig13-d.png} \label{fig:img13-d} }
	\caption{Contour pixel reconstruction. (\textbf{a}) SE; (\textbf{b}) NW; (\textbf{c}) SW; (\textbf{d}) NE.}
	\label{fig:image13}
\end{figure}

% The methods in figure \JHMEMO{13} are the basic methods for restoring the contour, but they have a problem in reconstructing the inner corner pixel using three or more representative points. Table \ref{table:ex_innerCornerMissing} and figure \JHMEMO{14} show cases of missing inner corner pixels using sequential representative points $R_i$, $R_{i+1}$, and $R_{i+2}$.

The methods in Figure \ref{fig:image13} are the basic methods employed to restore the contour, but they are problematic when used to reconstruct the inner-corner pixel using three or more representative points. Table \ref{table:ex_innerCornerMissing} and Figure \ref{fig:image14} show cases of the missing inner-corner pixels using sequential representative points $R_i$, $R_{i+1}$ and $R_{i+2}$.


 \begin{table}[H]
	\centering
	\begin{tabular}{ccc}
		\toprule
		\textbf{Case} & \boldmath{$(R_i, R_{i+1})$} & \boldmath{$(R_{i+1}, R_{i+2})$}\\
		\midrule
		1 & SE & NE \\
		2 & SE & SE \\
		3 & NE & SW \\
		4 & NE & NW \\
		5 & NW & NW \\
		6 & NW & SW \\
		7 & SW & SE \\
		8 & SW & NE \\
		\bottomrule
	\end{tabular}
	\caption{Examples of the inner corner missing.}
	\label{table:ex_innerCornerMissing}
\end{table}
%%% Image 13
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig14-a.png} \label{fig:img14-a} }
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig14-b.png} \label{fig:img14-b} }
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig14-c.png} \label{fig:img14-c} } 
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig14-d.png} \label{fig:img14-d} } //
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig14-e.png} \label{fig:img14-e} }
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig14-f.png} \label{fig:img14-f} }
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig14-g.png} \label{fig:img14-g} }
	\subfloat[]{ \includegraphics[width=0.24\textwidth]{4.Proposed/fig14-h.png} \label{fig:img14-h} }
	 
	\caption{Restoration cases for different sequences of representation points. (\textbf{a}--\textbf{h}) Case 1--8.}
	\label{fig:image14}
\end{figure}



% As shown in the figure, the three representative points cannot restore the inner corner pixel $P_m$. Therefore, if the three sequential points form one of the cases in table \ref{table:ex_innerCornerMissing}, $P_m$ of the middle representative point $R_{i+1}$ must be filled with black. 

As shown in Figure \ref{fig:image15}, the three representative points cannot restore the inner-corner pixel $P_m$. Therefore, if the three sequential points form one of the cases in Table \ref{table:ex_innerCornerMissing}, $P_m$ of the middle representative point $R_{i+1}$ must be filled with a dark color. 


\begin{figure}[H]
	\centering
	\includegraphics[width=0.2\textwidth]{5.ExperimentalResult/fig15.png}
	\caption{Problem with TPA. TPA must start with white left~(L), left-rear~(W) and right-rear~(R) pixels. However, generic inner contours cannot satisfy these criteria.}
	\label{fig:image15}
\end{figure}
% This reconstruction method is different from the restoration in the RD code method since the proposed method does not need the RD code but requires only representative points, and their saved sequence naturally replaces the RD code. Therefore, it requires a smaller memory size as compared to the RD code method.

This reconstruction method is different from the restoration approach in the RD code method because the proposed method does not need the RD code, but requires only representative points, and their saved sequence naturally replaces the RD code. Therefore, it requires a smaller memory size when compared to the RD code method.

% \subsubsubsection{Inner-outer corner restoration}
\paragraph{Inner-Outer Corner Restoration}

% The restored contour using only the representative points has no inner-outer corners since the inner-outer corner is not considered. For this reason, if there are inner-outer corner points in the data table, as shown in Table \ref{table:data_structure}, the inner-outer corners are generated by using the data $C_j$ with their point coordinates and types. If a pixel restored using the representative points is $o(x, y)$ and $O$ is restored contour, the function of the inner-outer corner restoration, $R_{IO}$, can be obtained as:

The restored contour obtained using only the representative points has no inner-outer corners because the inner-outer corner is not considered. For this reason, if there are inner-outer corner points in the data table, as shown in Table \ref{table:ex_innerCornerMissing}, the inner-outer corners are generated using the data $C_j$ with their point coordinates and types. If a pixel restored using the representative points is $o(x, y)$ and $O$ is the restored contour, the function of the inner-outer corner restoration, $R_{IO}$, can be obtained as:

\begin{equation}
R_{IO} = \begin{cases}
O - o(c_{j,x} - 0.5, c_{j,y} - 0.5) - o(c_{j,x} + 0.5, c_{j,y} + 0.5), \text{where\:} c_{j,T} = ``NW-SE'' \\
O - o(c_{j,x} - 0.5, c_{j,y} + 0.5) - o(c_{j,x} + 0.5, c_{j,y} - 0.5), \text{where\:} c_{j,T} = ``NE-SW'' \\
\end{cases}
\label{eq:r_io}
\end{equation}

%-----------------------------------------------------------------------------------------



%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

\section{Experimental Result}

% For comparing the proposed algorithm with conventional algorithms, we perform experiment for determining their accuracy, speed, and stored data size. Table \ref{table:exp_environment} shows the experimental environment.

To compare the proposed algorithm to conventional algorithms, we perform an experiment to determine their accuracy, speed and stored data size. Table \ref{table:exp_environment} shows the experimental environment.

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		 & \textbf{Desktop} \\ 
		\midrule
		CPU & Intel\textregistered~ Core\texttrademark~i7-2600K CPU @%pelase check the use of the @
		3.40 GHz \\
		Memory & 14.0 GB \\ 
		HDD%please define
		& Seagate 1 TB Momentus ST1000LM024 \\
		OS & Microsoft Windows 7 \\
		Development & Microsoft Visual Studio 2013 \\ 
		\bottomrule
	\end{tabular}
	\caption{Experimental environment.}
	\label{table:exp_environment}
\end{table}	

% We experimented on nine CCITT standard fax images with 200 DPI (dots per inch) \cite{Miyatake1997Contour}. All these images have $1,728 \times 2,339$ pixels and a file size of 11,842 KB. Table \ref{table:ccitt} shows the document type of these images and the total number of contour pixels. We used these large sized images because they a number of various types of contours, and they are useful to compare the efficiencies with regard to parameters such as processing time and accuracy of the trace results of the contour tracing algorithms.

We experimented on nine \hl{CCITT}
 standard fax images with 200 dots per inch (dpi) \cite{Miyatake1997Contour}. All of these images have $1728 \times 2339$ pixels and a file size of 11,842~KB. Table \ref{table:ccitt} shows the document type of these images and the total number of contour pixels. We used these large-sized images because they have various types of contours, which is useful when comparing the efficiencies with regard to parameters, such as processing time and the accuracy of the trace results of the contour-tracing~algorithms.

\begin{table}[H]
	\centering
	\begin{tabular}{ccc}
		\toprule
		\textbf{Index} & \textbf{Type} & \textbf{Total Number of Contour Pixels} \\
		\midrule
		1 & Business letter & 81,189 \\
		2 & Circuit diagram & 50,825 \\
		3 & Sales order table & 152,489 \\
		4 & French document & 312,812 \\
		5 & Technical paper & 157,377 \\
		6 & Technical graph & 98,579 \\
		7 & Japanese document & 283,717 \\
		8 & Handwritten memo & 97,031 \\
		9 & Facsimile test chart & 453,721 \\
		\bottomrule
	\end{tabular}
	\caption{\protect \hl{CCITT} fax standard images.}
	\label{table:ccitt}
\end{table}	

% In order to compare the proposed algorithm with conventional algorithms, we used the experimental method described in \cite{Danielsson1981Improvement} for determining the start pixels of the outer and inner contours of the images. In other words, whenever any untraced contour pixel is searched using a raster scan from the left-top to the right-bottom of the images, this pixel is regarded as the start pixel and the tracer starts contour tracing. If the contour is an outer contour, the tracer's initial direction is assigned as East (``E''). On the contrary, in the case of the inner contour of an object e.g., ``e,'' ``p,'' ``q,'' ``R,'' and ``o,'' the tracer's initial direction is assigned as West (``W''), as shown in figure \textcolor{red}{2 (a)}.

In order to compare the proposed algorithm to the conventional algorithms, we used the experimental method described in \cite{Danielsson1981Improvement} to determine the start pixels of the outer and inner contours of the images. In other words, whenever any untraced contour pixel is searched using a raster scan from the left top to the right bottom of the images, this pixel is regarded as the start pixel, and the tracer starts contour tracing. If the contour is an outer contour, the tracer's initial direction is assigned as east (``E''). On the contrary, in the case of the inner contour of an object, e.g., ``e,'' ``p,'' ``q,'' ``R'' and ``o,'' the tracer's initial direction is assigned as west (``W''). 

% In the experiments, we did not consider the TPA, since the initial condition of the TPA\cite{Ghuneim2015Contour,Pavlidis2012Algorithms} is that it must start with white left (``L''), left-rear (``W''), and right-rear (``R'') pixels, which is not satisfied in some of the inner contours. Figure \ref{fig:image15} shows an example of this violation. In our experimental situation, there were many cases in which such conditions were not satisfied; therefore, we could not perform identical experiments and meaningful data was not obtained for comparing the trace results with those of the other methods. 

In the experiments, we did not consider the TPA because the initial condition of the TPA is that it must start with white left (``L''), left-rear (``W'') and right-rear (``R'') pixels \cite{Ghuneim2015Contour}, which is not satisfied in some of the inner contours. Figure \ref{fig:image15} shows an example of this violation. In our experiments, there were many cases for which these conditions were not satisfied. Therefore, we could not perform identical experiments, and meaningful data were not obtained for comparing the trace results with those of the other methods. 

%%% Figure 15


\subsection{Accuracy}

% The accuracy of contour tracing involves determining how accurately the tracing algorithm traces, and we measure it by counting the number of pixels traced. Firstly, we apply each algorithm to the test images and mark the tracing on the images. Then we count all the marked contour pixels in the images. Therefore, even if a pixel is traced several times, it is counted only once. Table \JHMEMO{7} shows the results of the comparison between the proposed algorithm and the conventional ones. In the table, \JHMEMO{``total number''} implies the total number of contour pixels, including the inner corner, outer corner, inner-outer corner, and straight line pixels. In this result, the MNT and RSA traced the least number of pixels as contours because they could not trace the inner corner pixels. the SBF has inconsistencies with regard to the inner-outer corner and inner corner types; therefore, it traced lesser number of pixels as compared to the ISBF and proposed algorithm. Further, the MSBF has inner corner inconsistencies that are similar to those of the SBF; the MSBF traced lesser number of pixels as compared to the proposed algorithm and ISBF. The proposed algorithm shows that $99.5\%$ of the total contour pixels were found to be the same as those in the case of the ISBF and it has the maximum total number of traced contour pixels. In conclusion, the proposed algorithm produced the best results with regard to tracing accuracy.

%%% Table 7
%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-
The accuracy of contour tracing involves determining how accurately the tracing algorithm traces, and we measure it by counting the number of pixels traced. First, we apply each algorithm to the test images and mark the tracing on the images. Then, we count all of the marked contour pixels in the images. Therefore, even if a pixel is traced several times, it is counted only once. Table \ref{table:table7} shows the results of the comparison between the proposed algorithm and the conventional ones. In the table, ``total number'' implies the total number of ground truth contour pixels, including the inner corner, outer corner, inner-outer corner and straight-line pixels. The ground truth pixels are counted that are adjacent to the background pixels. In this result, MNT and RSA traced the least number of pixels as contours, because they could not trace the inner-corner pixels. SBF has inconsistencies with regard to the inner-outer corner and inner-corner types. Therefore, it traced fewer pixels when compared to ISBF and the proposed algorithm. Further, MSBF has inner-corner inconsistencies that are similar to those of SBF, and MSBF traced fewer pixels when compared to the proposed algorithm and ISBF. The proposed algorithm shows that $99.5\%$ of the total contour pixels were found to be the same as those in the case of ISBF, and it has the maximum total number of traced contour pixels. In conclusion, the proposed algorithm produced the best results with regard to tracing accuracy.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[H]
\centering
\caption{Comparison of traced contour pixels.}
\label{table:table7}
\resizebox{\textwidth}{!}{%

\begin{tabular}{cccccccccccccc}

\toprule

\multirow{2}{*}{\textbf{Image}} & \multirow{2}{*}{\textbf{Total Number}} & \multicolumn{2}{c}{\textbf{Proposed}} & \multicolumn{2}{c}{\textbf{ISBF}} & \multicolumn{2}{c}{\textbf{MSBF}} & \multicolumn{2}{c}{\textbf{SBF}} & \multicolumn{2}{c}{\textbf{MNT}} & \multicolumn{2}{c}{\textbf{RSA}} \\
\cmidrule(rl){3-4}\cmidrule(rl){5-6}\cmidrule(rl){7-8}\cmidrule(rl){9-10}\cmidrule(rl){11-12}\cmidrule(rl){13-14}
          &              & \textbf{Number}     & \textbf{\%}    & \textbf{Number}   & \textbf{\%}   & \textbf{Number   } & \textbf{\% }   & \textbf{Number}   & \textbf{\%}   & \textbf{Number}   & \textbf{\%}   & \textbf{Number}   & \textbf{\%}   \\

\midrule

\#1         & 81,189           & 81,188     & 100.0   & 81,188   & 100.0  & 73,743   & 90.8  & 73,613   & 90.7  & 65,503   & 80.7  & 65,503   & 80.7  \\
\#2         & 50,825           & 50,824     & 100.0   & 50,824   & 100.0  & 45,003   & 88.5  & 45,003   & 88.5  & 38,819   & 76.4  & 38,819   & 76.4  \\
\#3         & 152,489          & 152,487    & 100.0   & 152,487   & 100.0  & 139,589   & 91.5  & 139,589   & 91.5  & 126,414   & 82.9  & 126,414   & 82.9  \\
\#4         & 312,812          & 312,812    & 100.0   & 312,812   & 100.0  & 283,709   & 90.7  & 283,712   & 90.7  & 253,169   & 80.9  & 253,169   & 80.9  \\
\#5         & 157,377          & 157,374    & 100.0   & 157,374   & 100.0  & 142,447   & 90.5  & 142,453   & 90.5  & 127,306   & 80.9  & 127,306   & 80.9  \\
\#6         & 98,579           & 98,566     & 100.0   & 98,566   & 100.0  & 91,176   & 92.5  & 91,174   & 92.5  & 82,239   & 83.4  & 82,239   & 83.4  \\
\#7         & 283,717          & 283,551    & 99.9   & 283,551   & 99.9  & 264,108   & 93.1  & 264,067   & 93.1  & 238,533   & 84.1  & 238,533   & 84.1  \\
\#8         & 97,031           & 97,015     & 100.0   & 97,015   & 100.0  & 86,822   & 89.5  & 86,822   & 89.5  & 76,251   & 78.6  & 76,251   & 78.6  \\
\#9         & 453,721          & 445,975    & 98.3   & 445,972   & 98.3  & 417,687   & 92.1  & 417,735   & 92.1  & 361,439   & 79.7  & 361,439   & 79.7  \\
\midrule
Total        & 1,687,740         & 1,679,792   & 99.5   & 1,679,789  & 99.5  & 1,544,284  & 91.5  & 1,544,168  & 91.5  & 1,369,673  & 81.2  & 1,369,673  & 81.2 \\

\bottomrule

\end{tabular}
}
\end{table}


Figure \ref{fig:image16} shows the traced images resulting from MSBF and the proposed algorithm. As shown in Figure \ref{fig:img16-a}, MSBF was not able to trace some of the inner corner pixels, but our proposed method (Figure \ref{fig:img16-b}) was able to trace all of the corner-pixel types without any inconsistency. Moreover, as the proposed algorithm can classify each corner type, it can trace the selected type of contour pixels by omitting some cases, as shown in Figure \ref{fig:img9-b}. For example, if we remove the tracing Cases (1) and (6) from the other cases, we can obtain a result without inner-corner tracing, and it is the same as the result of MNT and RSA. Figure \ref{fig:img17-b} shows an image that was traced using the proposed algorithm without inner corners, and it shows that the image is consistently traced without inner corners.


%%% Figure 16
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{5.ExperimentalResult/fig16-a.png} \label{fig:img16-a} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{5.ExperimentalResult/fig16-b.png} \label{fig:img16-b} }

	\caption{Visual comparison of two contour-tracing methods. (\textbf{a}) MSBF; (\textbf{b}) the proposed method.}
	\label{fig:image16}
\end{figure}

% Figure 16 shows the traced images resulting from the MSBF and proposed algorithm. In this figure, (a) could not trace some of the inner corner pixels but (b) traced all the corner pixel types without any inconsistency. Moreover, as the proposed algorithm can classify each corner type, it can trace the selected type of contour pixels by omitting some cases, as shown in figure 9 (b). For example, if we remove the tracing cases (1) and (6) from the other cases, we can obtain a result without inner corner tracing, and it is the same as the result of the MNT and RSA. Figure \textcolor{red}{17 (b)} shows an image that is traced using the proposed algorithm without inner corners, and it shows that the image is consistently traced without inner corners.


%%% Figure 17
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{5.ExperimentalResult/fig17-a.png} \label{fig:img17-a} }
	\subfloat[]{ \includegraphics[width=0.3\textwidth]{5.ExperimentalResult/fig17-b.png} \label{fig:img17-b} }

	\caption{Visual comparison of two contour-tracing methods. (\textbf{a}) MNT; (\textbf{b}) the proposed method (without inner corners).}
	\label{fig:image17}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Speed}

% In order to measure the tracing time for each algorithm, we performed each algorithm 10 times per image and calculated the average time. We used the GetTickCount() function supported by Microsoft Visual C++ 6.0 to measure the processing time. Tables \textcolor{red}{8 and 9} show the average processing time of each algorithm used for tracing the images, and a linear model for estimating the process time as the number of traced pixels increases by using the least-square estimation (LSE) method. In the tables, the average processing time per traced contour pixel is obtained by dividing the total processing time by the total number of traced contour pixels. They are measured on the desktop and notebook separately.

In order to measure the tracing time for each algorithm, we performed each algorithm 20 times per image and calculated the average time. We used the \hl{\textit{cv2.getTickCount()}} function supported by \hl{\textit{OpenCV~3.0.0}} to measure the processing time. Table \ref{table:table8} shows the average processing time of each algorithm used for tracing the images and a linear model for estimating the process time as the number of traced pixels increases using the least-square estimation (LSE) method. In the table, we obtain the average processing time per traced contour pixel by dividing the total processing time by the total number of traced contour pixels.

% Moreover, figure \textcolor{red}{18} illustrates a graph that uses data from tables \textcolor{red}{7 to 9}. As shown in figures \textcolor{red}{18 (a) and (b)}, the proposed algorithm had the best performance in the case of the desktop, i.e., it had the least average processing time and showed the least increase in the ratio of process time to number of traced contour pixels, as shown in the LSE. In particular, although the proposed algorithm traced most of the numerous contour pixels in each image, it has the best or good performance when compared with the conventional algorithms. On the contrary, the SBF had the least average processing time for images and the least ratio from the LSE in the case of the notebook, and the proposed algorithm is second in rank based on the average processing time and LSE. from these experimental results the SBF is not the best algorithm for the note book because the ratio of the number of traced contour pixels is only approximately $92\%$ of the proposed algorithm. Due to this reason, the proposed algorithm has better performance than the other algorithms for the number of traced contour pixels and the processing time.

% \JHMEMO{
Figure \ref{fig:image18} illustrates a graph that uses data from Table \ref{table:table7}. As shown in Figure \ref{fig:image18}, the proposed algorithm has the best performance, \emph{i.e.}, it had the least average processing time and showed the smallest increase in the ratio of the process time to the number of traced contour pixels, as shown in the LSE. In particular, although the proposed algorithm traced most of the contour pixels in each image, it has the best or a good performance when compared to the conventional algorithms. Furthermore, the proposed algorithm shows better standard deviation results than the conventional algorithms. Therefore, the proposed algorithm has better performance than the other algorithms for the number of traced contour pixels and the processing time.

% }

%%% Table 8
%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

\definecolor{Gray}{gray}{0.85}

% \begin{table}[]
% \centering
% \caption{Speed Experimental Result (Unit: Seconds)}
% \label{table:table8}
% \resizebox{\textwidth}{!}{%
% % \begin{tabular}{p{0.14\textwidth}llllll}
% \begin{tabular}{p{0.14\textwidth}p{0.14\textwidth}p{0.14\textwidth}p{0.14\textwidth}p{0.14\textwidth}p{0.14\textwidth}p{0.14\textwidth}}
% \toprule

% Image               & Proposed Algorithm & ISBF   & MSBF   & SBF    & MNT    & RSA    \\
% \midrule
% \#1               & \cellcolor{Gray}0.00596     & 0.00622  & 0.00658  & 0.00626  & 0.00626  & 0.00750  \\
% \#2               & \cellcolor{Gray}0.00432     & 0.00562  & 0.00592  & 0.00468  & 0.00472  & 0.00560  \\
% \#3               & 0.00778     & 0.00808  & 0.00936  & \cellcolor{Gray}0.00752  & 0.00808  & 0.00940  \\
% \#4               & 0.01406     & 0.01500  & 0.01844  & \cellcolor{Gray}0.01346  & \cellcolor{Gray}0.01374  & 0.01842  \\
% \#5               & 0.00848     & 0.00908  & 0.01000  & 0.00866  & \cellcolor{Gray}0.00842  & 0.01000  \\
% \#6               & \cellcolor{Gray}0.00592     & 0.00684  & 0.00750  & 0.00658  & 0.00624  & 0.00686  \\
% \#7               & \cellcolor{Gray}0.01308     & 0.01470  & 0.01904  & 0.01376  & 0.01344  & 0.01748  \\
% \#8               & 0.00624     & 0.00688  & 0.00784  & \cellcolor{Gray}0.00622  & \cellcolor{Gray}0.00622  & 0.00656  \\
% \#9               & \cellcolor{Gray}0.01656     & 0.01874  & 0.02628  & 0.01778  & 0.01750  & 0.02188  \\
% \midrule
% Average              & \cellcolor{Gray}0.00916     & 0.01013  & 0.01233  & 0.00944  & 0.00940  & 0.01152  \\
% \midrule
% Average time per traced contour pixel & \cellcolor{Gray}$4.91\times 10^{-8}$     & $5.43\times 10^{-8}$  & $7.19\times 10^{-8}$  & $5.50\times 10^{-8}$  & $6.18\times 10^{-8}$  & $7.57\times 10^{-8}$  \\
% \midrule
% LSE               & $3.23\times 10^{-8} + 0.0031$     & $3.57\times 10^{-8} + 0.0035$ & $5.74\times 10^{-8} + 0.0025$ & $3.58\times 10^{-8} + 0.0033$ & $4.06\times 10^{-8} + 0.0032$ & $5.54\times 10^{-8} + 0.0031$ \\
% R-square             & 0.98117     & 0.98544  & 0.98838  & 0.98685  & 0.99461  & 0.97615  \\
% \bottomrule
% \multicolumn{7}{l}{\textsuperscript{*}\footnotesize{Results of best or faster than the proposed algorithm are marked as shadow.}}

% \end{tabular}
% }
% \end{table}

\begin{table}[H]
\scriptsize
\centering
\caption{Speed experimental result (units: seconds).}
\label{table:table8}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Image}} & \multicolumn{2}{c}{\textbf{Proposed Algorithm}} & \multicolumn{2}{c}{\textbf{ISBF}} & \multicolumn{2}{c}{\textbf{MSBF}} & \multicolumn{2}{c}{\textbf{SBF}} & \multicolumn{2}{c}{\textbf{MNT}} & \multicolumn{2}{c}{\textbf{RSA}} \\
\cmidrule(rl){2-3}\cmidrule(rl){4-5}\cmidrule(rl){6-7}\cmidrule(rl){8-9}
\cmidrule(rl){10-11}\cmidrule(rl){12-13}
 & \textbf{Mean} & \textbf{SD} & \textbf{Mean} & \textbf{SD} & \textbf{Mean} & \textbf{SD} & \textbf{Mean} & \textbf{SD} & \textbf{Mean} & \textbf{SD} & \textbf{Mean} & \textbf{SD}\\
 \midrule
\#1 & \cellcolor{Gray}0.00596 & \cellcolor{Gray}0.00032 & 0.00622 & 0.00069 & 0.00658 & 0.00074 & 0.00626 & 0.00060 & 0.00626 & 0.00054 & 0.00750 & 0.00058 \\
\#2 & \cellcolor{Gray}0.00432 & 0.00040 & 0.00562 & 0.00047 & 0.00592 & \cellcolor{Gray}0.00005 & 0.00468 & 0.00044 & 0.00472 & \cellcolor{Gray}0.00026 & 0.00560 & \cellcolor{Gray}0.00034 \\
\#3 & 0.00778 & 0.00046 & 0.00808 & \cellcolor{Gray}0.00036 & 0.00936 & 0.00085 & \cellcolor{Gray}0.00752 & 0.00069 & 0.00808 & \cellcolor{Gray}0.00033 & 0.00940 & 0.00066 \\
\#4 & 0.01406 & \cellcolor{Gray}0.00006 & 0.01500 & 0.00099 & 0.01844 & 0.00100 & \cellcolor{Gray}0.01346 & \cellcolor{Gray}0.00006 & \cellcolor{Gray}0.01374 & 0.00075 & 0.01842 & 0.00074 \\
\#5 & 0.00848 & \cellcolor{Gray}0.00038 & 0.00908 & 0.00080 & 0.01000 & 0.00086 & 0.00866 & 0.00054 & \cellcolor{Gray}0.00842 & 0.00075 & 0.01000 & 0.00065 \\
\#6 & \cellcolor{Gray}0.00592 & \cellcolor{Gray}0.00056 & 0.00684 & 0.00074 & 0.00750 & 0.00070 & 0.00658 & 0.00062 & 0.00624 & 0.00072 & 0.00686 & 0.00058 \\
\#7 & \cellcolor{Gray}0.01308 & 0.00068 & 0.01470 & 0.00096 & 0.01904 & 0.00110 & 0.01376 & 0.00091 & 0.01344 & \cellcolor{Gray}0.00058 & 0.01748 & 0.00072 \\
\#8 & 0.00624 & 0.00067 & 0.00688 & \cellcolor{Gray}0.00062 & 0.00784 & \cellcolor{Gray}0.00065 & \cellcolor{Gray}0.00622 & 0.00068 & \cellcolor{Gray}0.00622 & 0.00068 & 0.00656 & \cellcolor{Gray}0.00061 \\
\#9 & \cellcolor{Gray}0.01656 & 0.00079 & 0.01874 & \cellcolor{Gray}0.00063 & 0.02628 & \cellcolor{Gray}0.00053 & 0.01778 & 0.00086 & 0.01750 & 0.00089 & 0.02188 & 0.00093 \\
\midrule
Average & \cellcolor{Gray}0.00916 & & 0.01013 & & 0.01233 & & 0.00944 & & 0.00940 & & 0.01152 & \\
\midrule
\begin{tabular}[c]{@{}c@{}}  Average time per\\ traced contour pixel  \end{tabular} & \cellcolor{Gray}$4.91\times 10^{-8}$ & & $5.43\times 10^{-8}$ & & $7.19\times 10^{-8}$ & & $5.50\times 10^{-8}$ & & $6.18\times 10^{-8}$ & & $7.57\times 10^{-8}$ & \\
\midrule
LSE & \multicolumn{2}{l}{$3.23\times 10^{-8} + 0.0031$} & \multicolumn{2}{l}{$3.57\times 10^{-8} + 0.0035$} & \multicolumn{2}{l}{$5.74\times 10^{-8} + 0.0025$} & \multicolumn{2}{l}{$3.58\times 10^{-8} + 0.0033$} & \multicolumn{2}{l}{$4.06\times 10^{-8} + 0.0032$} & \multicolumn{2}{l}{$5.54\times 10^{-8} + 0.0031$} \\
R-square & 0.98117 & & 0.98544 & & 0.98838 & & 0.98685 & & 0.99461 & & 0.97615 & \\
\bottomrule
%\multicolumn{7}{c}{*~Results of best or faster speed/smaller standard deviation than the proposed algorithm are marked as shadow.} & & & & & & 
\end{tabular}
}\\
\begin{tabular}{ccccccccccccc}
\multicolumn{1}{c}{\footnotesize \hl{*}~Results of best or faster speed/smaller standard deviation than the proposed algorithm are marked with a shadow.}
\end{tabular}

\end{table}%Please add * in table.

%%% Image 18
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=.8\textwidth]{5.ExperimentalResult/fig18-a.png} \label{fig:img18-a} }\\
	\subfloat[]{ \includegraphics[width=.8\textwidth]{5.ExperimentalResult/fig18-b.png} \label{fig:img18-b} }

	\caption{Comparison of tracing times of the contour-tracing algorithms. (\textbf{a}) Processing time \emph{vs.} the number of contour pixels; (\textbf{b}) average contour tracing time \emph{vs.} images.}
	\label{fig:image18}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reduced Memory}

% The proposed algorithm does not save all the contour pixels, but it saves only the representative points and the inner-outer corner pixels. Table \textcolor{red}{10} shows the data size acquired from the above experiments on CCITT standard fax images. It shows the data sizes of traced contour pixels and its compressed data. The number of traced contour pixels $(A)$, which are same results from table \textcolor{red}{7} and the $C$ and $D$ in the table are numbers of the representative points and the inner-outer corner points of the traced contour pixels. $A$ and $C$ are the number of $(x, y)$ coordinates, and $D$ represents the number of inner-outer corners that comprise $(x, y)$ coordinates and the type of inner-outer corner. The benefit of storing only the representative points based on the vertex of the contour pixel is that it can dramatically reduce the data size. This experimental results showed that the proposed algorithm reduced the data size to {19~60\%} of the memory used when all the contour pixels were stored, as shown in Table \textcolor{red}{10}.

The proposed algorithm does not save all of the contour pixels, but it saves only the representative points and the inner-outer corner pixels. Table \ref{table:table10} shows the data size acquired from the above experiments performed using CCITT standard fax images. It shows the data sizes of traced contour pixels and their compressed data. The number of traced contour pixels (A), which are the same results from Table \ref{table:table7}, and $C$ and $D$ in the table indicate the number of representative points and inner-outer corner points of the traced contour pixels. $A$ and $C$ are the number of $(x, y)$ coordinates, and $D$ represents the number of inner-outer corners that comprise $(x, y)$ coordinates and the type of inner-outer corner. The benefit of storing only the representative points based on the vertex of the contour pixel is that it can significantly reduce the data size. The experimental results obtained show that the proposed algorithm reduced the data size to 19\%--60\% of the memory used when all of the contour pixels were stored, as shown in Table \ref{table:table10}.

%%% Table 10
%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

% \begin{table}[]
% \scriptsize
% \centering
% \caption{Comparison Between Total Contour Pixels and Representative Points.}
% \label{table:table10}
% \begin{tabularx}{1.0\textwidth}{rR{2.1cm}R{2.0cm}R{2.9cm}R{3.0cm}R{2.5cm}R{2cm}}

% \toprule
% \multirow{2}{*}{} & \multicolumn{2}{l}{Entire contour pixels} & \multicolumn{3}{l}{Compressed data} & \multirow{2}{*}{Ratio ($\%$) ($E / B \times 100$) }	\\
% 				 & \multicolumn{1}{l}{Number of contour pixels ($A$)} & Data size ($B = A \times 2$) & Number of representative points ($C$) & Number of Inner outer corner points ($D$) & Data size ($E = C \times 2 + D \times 3$) & \\
% \midrule
% \#1       & 81,188           & 162,376          & 21,206              & 61                  & 42,595 & 26.23 \\
% \#2       & 50,824           & 101,648          & 12,695              & 21                  & 25,453 & 25.04 \\
% \#3       & 152,487           & 304,974          & 30,181              & 53                  & 60,521 & 19.84 \\
% \#4       & 312,812           & 625,624          & 86,953              & 442  & 175,232 & 28.01 \\
% \#5       & 157,374           & 314,748          & 37,386              & 113  & 75,111 & 23.86 \\
% \#6       & 98,566           & 197,132          & 18,464              & 104  & 37,240 & 18.89 \\
% \#7       & 283,551           & 567,102          & 84,484              & 1,539 & 173,585 & 30.61 \\
% \#8       & 97,015           & 194,030          & 21,928              & 104  & 44,168 & 22.76 \\
% \#10      & 445,975           & 891,950          & 158,529              & 75,425 & 543,333 & 60.92 \\
% \bottomrule

% \end{tabularx}
% \end{table}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[H]
\centering
\caption{Comparison between total contour pixels and representative points.}
\label{table:table10}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccc}

\toprule

\multicolumn{1}{c}{} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Entire Contour Pixels}}}                           & \multicolumn{3}{c}{\multirow{2}{*}{\textbf{Compressed Data}}}               &\textbf{Ratio (\%)} \\

&&&&&& (\boldmath{$E / B \times 100$})            \\
\midrule

\multicolumn{1}{c}{} & Number of & Data size& Number of & Number of inner outer & Data size &        \\

\multicolumn{1}{c}{} & contour pixels ($A$) & ($B = A \times 2$) & representative points ($C$) & corner points ($D$) &($E = C \times 2 + D \times 3$) &         \\

\midrule
\#1        & 81,188                    & 162,376                   & 21,206       & 61         & 42,595       & 26.23                   \\
\#2        & 50,824                    & 101,648                   & 12,695       & 21         & 25,453       & 25.04                   \\
\#3        & 152,487                   & 304,974                   & 30,181       & 53         & 60,521       & 19.84                   \\
\#4        & 312,812                   & 625,624                   & 86,953       & 442         & 175,232       & 28.01                   \\
\#5        & 157,374                   & 314,748                   & 37,386       & 113         & 75,111       & 23.86                   \\
\#6        & 98,566                    & 197,132                   & 18,464       & 104         & 37,240       & 18.89                   \\
\#7        & 283,551                   & 567,102                   & 84,484       & 1,539       & 173,585       & 30.61                   \\
\#8        & 97,015                    & 194,030                   & 21,928       & 104         & 44,168       & 22.76                   \\
\#10       & 445,975                   & 891,950                   & 158,529       & 75,425       & 543,333       & 60.92                   \\
\bottomrule
\end{tabular}
}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Restoration}

% Figure \ref{fig:image19} shows an example of retrieval points and a restoration result obtained by using the proposed restoration algorithm. Figure \ref{fig:img19-a} is an example image that has all the contour pixel types, and it depicts the representative points and inner-outer corner points for contour description and restoration. This image has two contours, namely, an outer contour and an inner contour that includes two inner-outer corners. Table \textcolor{red}{11} describes these data and figure \textcolor{red}{18 (b)} shows the restoration results, which are retrieved using the data from table \textcolor{red}{11}. In the figure, the restored contour accurately represents the original contour pixels.

Figure \ref{fig:image19} shows an example of the retrieval points and a restoration result obtained using the proposed restoration algorithm. Figure \ref{fig:img19-a} is an example image that has all of the contour pixel types, and it depicts the representative points and inner-outer corner points for contour description and restoration. This image has two contours, namely an outer contour and an inner contour that includes two inner-outer corners. Table \ref{table:table11} describes these data, and Figure \ref{fig:img19-b} shows the restoration results, which were retrieved using the data from Table \ref{table:table11}. In the figure, the restored contour accurately represents the original contour pixels.

%%% Image 19
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=7cm, height=6cm]{5.ExperimentalResult/fig19_a.png} \label{fig:img19-a} }
	\subfloat[]{ \includegraphics[width=7cm, height=5.25cm]{5.ExperimentalResult/fig19_b.png} \label{fig:img19-b} }
	 
	\caption{Example of the restoration of contour pixels. (\textbf{a}) The original image and its saved points for restoration; (\textbf{b}) restoration by the saved data.}
	\label{fig:image19}
\end{figure}

%%% Table 11
%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-
Figure \ref{fig:image20} shows the result obtained for the CCITT \#1 image using the proposed restoration algorithm. Figure \ref{fig:img20-a} represents the contour-tracing result, and Figure~\ref{fig:img20-b} depicts the result of the restoration from the compressed contour data. To verify the identity, we compared the contour pixels of the two images and found that they are identical with regard to the number of contour pixels and the pixel coordinates, \emph{i.e.}, the contour pixels in the restoration result are the same as the original contour pixels. As shown in Figures \ref{fig:image19} and \ref{fig:image20}, these experiments proved that the proposed algorithm could trace the inner and outer contours. Further, it was able to store the results using less memory by storing only the representative points and inner-outer corner points instead of all of the contour pixels; moreover, it could correctly restore all of the contour pixels from the compressed data. Besides, as shown in \cite{Miyatake1997Contour}, compressed data based on vertex contours guarantee precise enlarging.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[H]
\centering
\caption{Saved data.}
\label{table:table11}
\resizebox{0.7\textwidth}{!}{%

\begin{tabular}{ccccccccccc}
\toprule
\multirow{3}{*}{\textbf{i}} & \multicolumn{5}{c}{\textbf{Contour \#1. (Outer Contour)}}       & \multicolumn{5}{c}{\textbf{Contour \#2. (Inner Contour)}}       \\ \cmidrule(rl){2-6}\cmidrule(rl){7-11}
         & \multicolumn{2}{c}{\boldmath{$R_i$}} & \multicolumn{2}{c}{\boldmath{$C_i$}} &     & \multicolumn{2}{c}{\boldmath{$R_i$}} & \multicolumn{2}{c}{\boldmath{$C_i$}} &     \\
         & \boldmath{$x$}     & \boldmath{$y$}     & \boldmath{$x$}     & \boldmath{$y$}     & \textbf{type}  & \boldmath{$x$}     & \boldmath{$y$}     & \boldmath{$x$}     & \boldmath{$y$}     & \textbf{type}  \\
\midrule 
1        & 3.5    & 1.5    & 9.5    & 9.5    & NE-SW  & 5.5    & 9.5    & 5.5    & 9.5    & NW-SE  \\
2        & 5.5    & 3.5    & 5.5    & 9.5    & NW-SE  & 9.5    & 5.5    & 9.5    & 9.5    & NE-SW  \\
3        & 9.5    & 1.5    &      &      &     &      &      &      &      &     \\
4        & 11.5   & 3.5    &      &      &     &      &      &      &      &     \\
5        & 13.5   & 7.5    &      &      &     &      &      &      &      &     \\
6        & 11.5   & 9.5    &      &      &     &      &      &      &      &     \\
7        & 9.5    & 11.5   &      &      &     &      &      &      &      &     \\
8        & 5.5    & 9.5    &      &      &     &      &      &      &      &     \\
9        & 3.5    & 7.5    &      &      &     &      &      &      &      &     \\
10       & 1.5    & 3.5    &      &      &     &      &      &      &      &     \\
\bottomrule
\end{tabular}
}
\end{table}

% Figure \ref{fig:image20} shows the result for the CCITT \#1 image using the proposed restoration algorithm. Figure \ref{fig:img20-a} represents the result of contour tracing and \ref{fig:img20-b} depicts the result of restoration from the compressed contour data. To verify the identity, we compared the contour pixels of the two images and they are identical with regard to the number of contour pixels and the pixel coordinates, i.e., the contour pixels in the restoration result is the same as the original contour pixels. As shown in figures \ref{fig:image20} \textcolor{red}{figures 19 and 20}, these experiments proved that the proposed algorithm could trace the inner and outer contours, and it could store the results using lesser memory by storing only the representative points and inner-outer corner points instead of all the contour pixels; moreover, it could accurately restore all the contour pixels correctly from the compressed data. Besides, as shown in \cite{Miyatake1997Contour}, compressed data based on vertex contours guarantee precise enlarging.


%%% Image 20
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=0.5\textwidth]{5.ExperimentalResult/fig20_a.png} \label{fig:img20-a} }
	\subfloat[]{ \includegraphics[width=0.5\textwidth]{5.ExperimentalResult/fig20_b.png} \label{fig:img20-b} }
	 
	\caption{Result of the experiment for CCITT \#1. Contour pixels are shown in black, original image pixels in grey. (\textbf{a}) Result of contour tracing; (\textbf{b}) result of contour restoration.}
	\label{fig:image20}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Limitations}

In the experiments, there were missing contour pixels that did not satisfy the experimental conditions described in \cite{Miyatake1997Contour}. In Figure \ref{fig:image21}b,c, there are eight untraced contour pixels because the horizontal scan line cannot find a starting pixel for the contour under certain conditions. In other words, because the scan line seeks an untraced black pixel with an adjacent white pixel on the horizontal line, if the untraced contour pixel that is between two black pixels in the horizontal direction has an adjacent white pixel in the vertical and/or diagonal direction, the untraced contour pixel cannot be considered as the starting pixel. Therefore, as shown in Table \ref{table:table7}, the missing contour pixels remain after running the proposed algorithm, and the untraced contour pixels of other algorithms are included in the missing contour pixels for the same reason. In particular, Image \#9 has the largest number of missing contour pixels because it has many one-pixel-sized chessboard patterns that comprise inner-outer corner pixels. The chessboard pattern consists of one-pixel-sized inner-outer corner pixels, which tend to cause the missing start-pixel problem. 

%%% Image 21
\begin{figure}[H]
	\centering
	\subfloat[]{ \includegraphics[width=4.5cm, height=4.5cm]{5.ExperimentalResult/fig21-a.png} \label{fig:img21-a} }
	\subfloat[]{ \includegraphics[width=4.5cm, height=4.5cm]{5.ExperimentalResult/fig21-b.png} \label{fig:img21-b} }
	\subfloat[]{ \includegraphics[width=4.5cm, height=4.5cm]{5.ExperimentalResult/fig21-c.png} \label{fig:img21-c} }
	 
	\caption{Example of untraced contour pixels caused by missing starting pixel CCITT Image \#9 From $(1093, 1766)$ to $(1108, 1780)$. (\textbf{a}) Original image; (\textbf{b}) traced by ISBF; (\textbf{c}) traced by the proposed~algorithm.}
	\label{fig:image21}
\end{figure}

To overcome the problem, we applied an eight-connection mask to the images to obtain the starting pixel, but the mask required many operations. In other words, we attempted to measure the performance of multi-direction scanning in order to eliminate the missing contour-pixel problem by using vertical and horizontal scans instead of an eight-connection mask operation. Table \ref{table:table12} shows the increase in the number of pixels traced using bidirectional scanning, and Table \ref{table:table13} describes the processing time for this method. Moreover, Figure \ref{fig:image22} shows the tracing result that was obtained using the proposed algorithm based on bidirectional scanning, and it shows that seven of the missing pixels are traced, but one diagonal connective-contour pixel \hl{(A)} remained untraced. 


%%% Image 22
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{5.ExperimentalResult/fig22.png}
	\caption{Result of the proposed algorithm by using bidirectional scanning.}
	\label{fig:image22}
\end{figure}

% In the experiments, there were some missing contour pixels that did not satisfy the experimental conditions described in \cite{Danielsson1981Improvement}. In figures \ref{fig:img21-b} and \ref{fig:img21-c}, there are eight untraced contour pixels because the horizontal scan line cannot find a starting pixel for the contour under some conditions. In other words, since the scan line seeks an untraced black pixel with an adjacent white pixel on the horizontal line, if the untraced contour pixel which is between two black pixels in the horizontal direction has an adjacent white pixel in the vertical and/or diagonal direction, the untraced contour pixel cannot be considered as the starting pixel. Therefore, as shown in \textcolor{red}{Table 7}, the missing contour pixels remain after running the proposed algorithm and the untraced contour pixels of other algorithms are also included in the missing contour pixels due to the same problem. In particular, image \#9 has the largest number of missing contour pixels because it has many one-pixel-sized chessboard patterns that comprise inner-outer corner pixels. The chessboard pattern consists of one-pixel-sized inner-outer corner pixels, which tend to cause the missing start pixel problem. 


% To overcome the problem, we can apply an 8 connection mask to the images for obtaining the starting pixel, but the mask requires many operations. In other words, we made an attempt to measure the performance of multi-direction scanning for eliminating the missing contour pixels problem by using vertical and horizontal scans instead of 8 connection mask operation. \textcolor{red}{Table 12} shows the increase in the number of pixels traced by using bidirectional scanning, and \textcolor{red}{Table 13} describes the processing time for this method. Moreover, figure \ref{fig:image22} shows the tracing result obtained by using the proposed algorithm based on bidirectional scanning, and it shows that seven of the missing pixels are traced but still one diagonal connective contour pixel \textcolor{red}{(A)} is untraced. 


%%% Table 13


% In the above tables, bidirectional scanning slightly increases the number of traced contour pixels, but their processing time increases dramatically. Moreover, the proposed algorithm shows acceptable performance in terms of accuracy $(99.5\%)$, although only unidirectional scanning is performed. Hence, unidirectional scanning based on the proposed algorithm is sufficient for application to contour tracing under the condition that a relatively small number of objects are present and real-time tracing such as AR, MR, and recognition-image-based code is performed on small-scale images such as those in a mobile computing environment.

In the above tables, bidirectional scanning slightly increases the number of traced contour pixels, but their processing time increases significantly. Moreover, the proposed algorithm shows acceptable performance in terms of accuracy $(99.5\%)$, although we performed only unidirectional scanning. Hence, unidirectional scanning based on the proposed algorithm is sufficient for the application to contour tracing under the condition that relatively few objects are present, and we performed real-time tracing, such as AR, MR and recognition-image-based code on small-scale images, such as those in a mobile computing environment.

%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table}[H]
\centering
\caption{Increases of pixels traced by the bidirectional scan from the one-directional scan.}
\label{table:table12}

\resizebox{\textwidth}{!}{%

\begin{tabular}{cccccccccccccc}
\toprule
   & \multirow{2}{*}{\textbf{Total Number}} & \multicolumn{2}{c}{\textbf{Proposed}} & \multicolumn{2}{c}{\textbf{ISBF}} & \multicolumn{2}{c}{\textbf{MSBF}} & \multicolumn{2}{c}{\textbf{SBF}} & \multicolumn{2}{c}{\textbf{MNT}} & \multicolumn{2}{c}{\textbf{RSA}} \\
   \cmidrule(rl){3-4}\cmidrule(rl){5-6}\cmidrule(rl){7-8}\cmidrule(rl){9-10}\cmidrule(rl){11-12}\cmidrule(rl){13-14}
   
   
   &              & \textbf{Number}   & \textbf{\%}     & \textbf{Number  } &\textbf{\%}    & \textbf{Number}   & \textbf{\%} &\textbf{Number}   &\textbf{\%}   & \textbf{Number}   & \textbf{\%}   & \textbf{Number}   &\textbf{\%}   \\ \midrule
\#1 & 81,189           & 1      & 0.001   & 1     & 0.001   & 1     & 0.001   & 1     & 0.001  & 0     & 0.000  & 0     & 0.000  \\
\#2 & 50,825           & 0      & 0.000   & 0     & 0.000   & 0     & 0.000   & 0     & 0.000  & 0     & 0.000  & 0     & 0.000  \\
\#3 & 152,489          & 2      & 0.001   & 2     & 0.001   & 3     & 0.002   & 3     & 0.002  & 1     & 0.001  & 1     & 0.001  \\
\#4 & 312,812          & 0      & 0.000   & 0     & 0.000   & 0     & 0.000   & 0     & 0.000  & 0     & 0.000  & 0     & 0.000  \\
\#5 & 157,377          & 3      & 0.002   & 3     & 0.002   & 0     & 0.000   & 0     & 0.000  & 0     & 0.000  & 0     & 0.000  \\
\#6 & 98,579           & 7      & 0.007   & 7     & 0.007   & 8     & 0.008   & 8     & 0.008  & 1     & 0.001  & 1     & 0.001  \\
\#7 & 283,717          & 120     & 0.042   & 120    & 0.042   & 67     & 0.024   & 67     & 0.024  & 92     & 0.032  & 92     & 0.032  \\
\#8 & 97,031           & 12     & 0.012   & 12     & 0.012   & 15     & 0.015   & 15     & 0.015  & 1     & 0.001  & 1     & 0.001  \\
\#9 & 453,721          & 7075    & 1.559   & 7076   & 1.560   & 433    & 0.095   & 444    & 0.098  & 3414   & 0.752  & 3414   & 0.752  \\
Total & 1,687,740         & 7220    & 0.428   & 7221   & 0.428   & 527    & 0.031   & 538    & 0.032  & 3509   & 0.208  & 3509   & 0.208  \\ 
\bottomrule
\end{tabular}
}
\end{table}

%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[H]
\centering
\caption{Computational time traced bidirectionally.}
\label{table:table13}

\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccccccccccc}
\toprule
\multirow{3}{*}{\textbf{Image}} & \multicolumn{2}{c}{\textbf{Proposed}} & \multicolumn{2}{c}{\textbf{ISBF}}         & \multicolumn{2}{c}{\textbf{MSBF}}   & \multicolumn{2}{c}{\textbf{SBF}}   & \multicolumn{2}{c}{\textbf{MNT}} & \multicolumn{2}{c}{\textbf{RSA}}   \\
\cmidrule(rl){2-3}\cmidrule(rl){4-5}\cmidrule(rl){6-7}\cmidrule(rl){8-9}
\cmidrule(rl){10-11}\cmidrule(rl){12-13}
          & \textbf{Processing}  & \multirow{2}{*}{\textbf{Ratio}} & \textbf{Processing} & \multirow{2}{*}{\textbf{Ratio}} & \textbf{Processing} &\multirow{2}{*}{\textbf{Ratio}} & \textbf{Processing} & \multirow{2}{*}{\textbf{Ratio}} & \textbf{Processing } &\multirow{2}{*}{\textbf{Ratio}} & \textbf{Processing} & \multirow{2}{*}{\textbf{Ratio}} \\
 
           & \textbf{Time}  & & \textbf{Time} & & \textbf{Time} & & \textbf{Time} & & \textbf{Time} & & \textbf{Time} & \\ \midrule          
                    
                              
                                                 
          
\#1         & 0.064      & 2.1  & 0.067     & 2.2 & 0.074     & 2.2 & 0.064     & 2.0 & 0.067     & 2.1 & 0.069     & 1.8 \\
\#2         & 0.061      & 2.8  & 0.063     & 2.2 & 0.061     & 2.1 & 0.056     & 2.4 & 0.061     & 2.6 & 0.067     & 2.4 \\
\#3         & 0.072      & 1.9  & 0.083     & 2.1 & 0.084     & 1.8 & 0.077     & 2.0 & 0.077     & 1.9 & 0.083     & 1.8 \\
\#4         & 0.11       & 1.6  & 0.116     & 1.5 & 0.131     & 1.4 & 0.109     & 1.6 & 0.111     & 1.6 & 0.135     & 1.5 \\
\#5         & 0.078      & 1.8  & 0.084     & 1.9 & 0.089     & 1.8 & 0.078     & 1.8 & 0.078     & 1.9 & 0.089     & 1.8 \\
\#6         & 0.065      & 2.2  & 0.073     & 2.1 & 0.075     & 2.0 & 0.064     & 1.9 & 0.07     & 2.2 & 0.072     & 2.1 \\
\#7         & 0.108      & 1.7  & 0.111     & 1.5 & 0.134     & 1.4 & 0.11     & 1.6 & 0.113     & 1.7 & 0.125     & 1.4 \\
\#8         & 0.069      & 2.2  & 0.07     & 2.0 & 0.074     & 1.9 & 0.067     & 2.2 & 0.067     & 2.2 & 0.075     & 2.3 \\
\#9         & 0.128      & 1.5  & 0.139     & 1.5 & 0.172     & 1.3 & 0.127     & 1.4 & 0.13     & 1.5 & 0.147     & 1.3 \\
Total        & 0.755      & 1.8  & 0.806     & 1.8 & 0.894     & 1.6 & 0.752     & 1.8 & 0.774     & 1.8 & 0.862     & 1.7 \\
\bottomrule

\end{tabular}
}
\end{table}


%-----------------------------------------------------------------------------------------



%!TEX root = ../Fast_Contour_Tracing_Algorithm.tex
% -*- root: ../Fast_Contour_Tracing_Algorithm.tex -*-

\section{Conclusions}

% In this paper, we proposed a contour tracing algorithm to trace contours in low-performance devices such as mobile phones, PDAs, and embedded devices that have a processor with limited computational capability and a small memory capacity. The proposed algorithm traces contour pixels based on the pixel following method, and it can also convert the contour information to compressed data and accurately restore it to the original contour by using the vertex following method. The proposed algorithm repeatedly executes the two tracing stages. first, the tracer moves to the next pixel based on its left and left-rear pixels. Next, it moves based on the front and front-left pixels. With these pixel pairs, the contour pixel can be classified into four types, namely, inner corner, outer corner, inner-outer corner, and straight line, and 
% this classification is used to reduce the computational time for tracing the next contour pixel without duplicated detection of the background pixel. Moreover, based on the classified cases, we can determine the representative points and the inner-outer corner points that are based on the coordinates of the vertices, and we can store the contour data as points in order to reduce the data size. In addition, we proposed a restoration algorithm to retrieve all the contour pixels from the representative points and the inner-outer corner points. It performs accurate restoration and it can also restore the inner-outer corners that were not considered in conventional algorithms such as the RD code method and the PXY method. Another characteristic of the proposed algorithm is that it can trace the desired type of connectivity since it is able to distinguish between the connection types of the contour pixels. For example, the proposed algorithm may trace without inner corners, which is similar to the performance of MNT and RSA.

In this paper, we proposed a contour-tracing algorithm to trace contours in low-performance devices, such as mobile phones, PDAs and embedded devices that have a processor with limited computational capability and a small memory capacity. The proposed algorithm traces contour pixels based on the pixel-following method, and it can also convert the contour information to compressed data and accurately restore it to the original contour using the vertex-following method. The proposed algorithm repeatedly executes the two tracing stages. First, the tracer moves to the next pixel based on its left and left-rear pixels. Next, it moves based on the front and front-left pixels. With these two tracing stages, the proposed algorithm extracts two contiguous contour pixels together. Because the proposed algorithm traces contiguous pixel pairs in a single step, there are more possible cases of the form a contour can take. Therefore, the classification of the contour is more complicated than the conventional algorithms. On the other hand, this classification actually takes less time to compute, because it reduces the duplicated detection of the background pixels. Moreover, based on the classified cases, we can determine the representative points and the inner-outer corner points that are based on the coordinates of the vertices, and we can store the contour data as points in order to reduce the data size. In addition, we proposed a restoration algorithm to retrieve all of the contour pixels from the representative points and the inner-outer corner points. The proposed algorithm performs accurate restoration, and it can restore the inner-outer corners that were not considered in conventional algorithms, such as the RD code method and the PXY method. Another characteristic of the proposed algorithm is that it can trace the desired type of connectivity because it is able to distinguish between the different types of connections of the contour pixels. For example, the proposed algorithm may trace without inner corners, which is similar to the performances of MNT and RSA.

% Because the proposed algorithm traces contiguous pixel pairs in a single step, there are more possible cases of the form a contour can take. Therefore the classification step of the contour may be more complicated than the conventional algorithms. On the other hand, this classification process actually takes less time to compute and trace the next contour pixel by reducing the number needed to detect background pixel since it checks values of two pixels at once. <- 정현이

% Although the entire contour cases are more sophisticated than the conventional algorithms, the proposed algorithm traces 

% Although the proposed algorithm traces two contour pixels at once, 

% 연속된 점을 


% % 이렇게 두 점을 한번에 trace 하면서 각 contour 픽셀을 4가지 타입으로 구분한다. 제안하는 방법은, 8가지 케이스로 2 점을 한 stage 에 trace 하기 때문에 설계는 복잡하지만, backgroun pixel 의 중복 detection을 줄일 수 있기 때문에 기존 알고리즘들에 비하여 높은 성능을 제공한다. 


% With these pixel pairs, the contour pixels can be classified as four types, namely, inner corner, outer corner, inner-outer corner, and straight line. 

% , and we used these classifications to reduce the computational time required for tracing the next contour pixel without duplicated detection of the background pixel. Moreover, based on the classified cases, we can determine the representative points and the inner-outer corner points that are based on the coordinates of the vertices, and we can store the contour data as points in order to reduce the data size. In addition, we proposed a restoration algorithm to retrieve all of the contour pixels from the representative points and the inner-outer corner points. The proposed algorithm performs accurate restoration, and it can restore the inner-outer corners that were not considered in conventional algorithms such as the RD code method and the PXY method. Another characteristic of the proposed algorithm is that it can trace the desired type of connectivity because it is able to distinguish between the different types of connections of the contour pixels. For example, the proposed algorithm may trace without inner corners, which is similar to the performances of MNT and RSA.

% We experimented with regard to three aspects-accuracy, speed, and saving data. In the results of the experiments, the proposed algorithm had the best performance with regard to the accuracy of contour tracing, i.e., it traced the largest number of contour pixels among the algorithms. Moreover, it showed the least average processing time per contour pixel and good performance with respect to the processing time of each image and the LSE. For this reason, it has reasonable performance and based on accuracy and processing time, it is regarded as the best among the algorithms. The proposed algorithm showed good performance with regard to not only the accuracy and speed but also the memory consumption. It stored only the representative points and inner-outer corner points; therefore, it reduced the memory consumption. Besides, the proposed restoration algorithm successfully retrieved all the contour pixels from the compressed data. Therefore, the proposed algorithm shows improved accuracy and fast processing of contour tracing, low memory consumption for saving the contour, and good restoration ability.

We performed experiments with regard to three aspects: accuracy, speed and saving data. From the experiment results, the proposed algorithm had the best performance with regard to the accuracy of contour tracing, \emph{i.e.}, of all the algorithms, it traced the largest number of contour pixels. Moreover, it had the smallest average processing time per contour pixel and good performance with respect to the processing time of each image and the LSE. For this reason, it is considered to have reasonable performance, and based on its accuracy and processing time, it is regarded as the best of the different algorithms. In addition to the accuracy and speed, the proposed algorithm exhibited good performance with regard to the memory consumption. It stored only the representative points and inner-outer corner points, thus reducing the memory consumption. Besides, the proposed restoration algorithm successfully retrieved all of the contour pixels from the compressed data. Therefore, the proposed algorithm shows improved accuracy and fast processing of contour tracing, low memory consumption for saving the contour and good restoration ability.
%-----------------------------------------------------------------------------------------

\vspace{6pt} 
\acknowledgments{\textbf{Acknowledgments:} This work was supported by the National Research Foundation of Korea~(NRF) grant funded by the Korea government~(MEST) (Nos. NRF-2012R1A2A2A01014499 and NRF-2015R1A2A1A10055673).}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\authorcontributions{\textbf{Author Contributions:} Jonghoon Seo contributed to the design of the contour following algorithm and the implementation of the proposed algorithm. Seungho Chae contributed to the design of the contour restoration method and was in charge of the experiment process. Jinwook Shim and Dongchul Kim contributed to the writing and revising of the paper. Cheolho Cheong guided the conventional contour following algorithms and helped to design the proposed algorithm. Tack-Don Han guided the research direction and verified the research results. All authors made substantial contributions to the writing and revision of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\conflictofinterests{\textbf{Conflicts of Interest:} The authors declare no conflict of interest.}

%=================================================================
% References: Variant A
%=================================================================
% Back Matter (References and Notes)
%----------------------------------------------------------
% Style and layout of the references
% \bibliographystyle{mdpi}
% \makeatletter
% \renewcommand\@biblabel[1]{#1. }
% \makeatother

% \begin{thebibliography}{999} % if there are less than 10 entries, enter a one digit number

% % Reference 1
% \bibitem{ref-journal}
% Lastname, F.; Author, T. The title of the cited article. {\em Journal Abbreviation} {\bf 2008}, {\em 10}, 142-149.

% % Reference 2
% \bibitem{ref-book}
% Lastname, F.F.; Author, T. The title of the cited contribution. In {\em The Book Title}; Editor, F., Meditor, A., Eds.; Publishing House: City, Country, 2007; pp. 32-58.

% \end{thebibliography}

%=================================================================
% References: Variant B
%=================================================================
% Use the following option to include external BibTeX files:
\bibliographystyle{mdpi}
\renewcommand\bibname{References}
%%MDPI internal note: new layout%% redefinition removed

\begin{thebibliography}{999} 

\bibitem[Mcqueen(2004)]{Mcqueen2004Contour}
Mcqueen, W.A.
\newblock Contour tracing and boundary detection for object identification in a
 digital image.
\newblock Patent United States 6674904, 6 January 2004.
%Please add patent date.

\bibitem[Pratt()]{Pratt????Digital}
Pratt, W.
\newblock {\em Digital Image Processing}; \hl{Wiley: City, Country}, 1978.
%Please add the publisher’s location (city, country).

\bibitem[Gose \em{et~al.}(1996)Gose, Johnsonbaugh, and Jost]{Gose1996Pattern}
Gose, E.; Johnsonbaugh, R.; Jost, S.
\newblock {\em Pattern Recognition and Image Analysis}; Prentice-Hall, Inc.:
 Upper Saddle River, NJ, USA, 1996.

\bibitem[Pitas(2000)]{Pitas2000Digital}
Pitas, I.
\newblock {\em Digital Image Processing Algorithms and Applications}; \hl{John
 Wiley \& Sons: City, Country}, 2000.

\bibitem[Das \em{et~al.}(1990)Das, Paulik, and Loh]{Das1990Bivariate}
Das, M.; Paulik, M.; Loh, N.
\newblock A bivariate autoregressive technique for analysis and classification
 of planar shapes.
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}
 {\bf 1990}, {\em 12},~97--103.

\bibitem[Papert(1973)]{Papert1973Uses}
Papert, S.
\newblock Uses of Technology to Enhance Education. Available online: http://hdl.handle.net/1721.1/6213 (accessed on \hl{Day Month Year}).

\bibitem[Cheong and Han(2006)]{Cheong2006Improved}
Cheong, C.; Han, T.D.
\newblock Improved simple boundary following algorithm.
\newblock {\em J. Korea Inf. Sci. Soc. Softw.
 Appl.} {\bf 2006}, {\em 33},~427--439.

\bibitem[Mirante and Weingarten(1982)]{Mirante1982Radial}
Mirante, A.; Weingarten, N.
\newblock The Radial Sweep Algorithm for Constructing Triangulated Irregular
 Networks. {\em IEEE Comput. Graphics Appl.} {\bf 1982}, {\em 2}, 11--21.

\bibitem[Pavlidis(2012)]{Pavlidis2012Algorithms}
Pavlidis, T.
\newblock {\em Algorithms for Graphics and Image Processing}; \hl{Springer Science
 \& Business Media: City, Country}, 2012.

\bibitem[Aroca \em{et~al.}(2013)Aroca, Gomes, Dantas, Calbo, and
 Gonçalves]{Aroca2013Wearable}
Aroca, R.V.; Gomes, R.B.; Dantas, R.R.; Calbo, A.G.; Gonçalves, L.M.G.
\newblock A Wearable Mobile Sensor Platform to Assist Fruit Grading.
\newblock {\em Sensors} {\bf 2013}, {\em 13},~6109--6140.

\bibitem[Wakaumi and Nagasawa(2006)]{Wakaumi20062D}
Wakaumi, H.; Nagasawa, C.
\newblock A 2D Ternary Barcode Detection System with a Dual Threshold.
\newblock In Proceedings of the 5th IEEE Conference on Sensors, Daegu, Korea, 22--25 October 2006; pp. 1511--1514.

\bibitem[Brodić \em{et~al.}(2010)Brodić, Milivojević, and
 Milivojević]{Brodic2010Basic}
Brodić, D.; Milivojević, D.R.; Milivojević, Z.
\newblock Basic Test Framework for the Evaluation of Text Line Segmentation and
 Text Parameter Extraction.
\newblock {\em Sensors} {\bf 2010}, {\em 10},~5263--5279.

\bibitem[Kim \em{et~al.}(2006)Kim, Seo, and Wang]{Kim2006Rapid}
Kim, J.H.; Seo, K.S.; Wang, J.
\newblock Rapid diagnostic barcode system for codetection of multiple protein
 markers.
\newblock {\em IEEE Sens. J.} {\bf 2006}, {\em 6},~248--253.

\bibitem[Tian \em{et~al.}(2010)Tian, Guan, and Wang]{Tian2010RealTime}
Tian, Y.; Guan, T.; Wang, C.
\newblock Real-Time Occlusion Handling in Augmented Reality Based on an Object
 Tracking Approach.
\newblock {\em Sensors} {\bf 2010}, {\em 10},~2885--2900.

\bibitem[Zhang \em{et~al.}(2012)Zhang, Zhao, and Lei]{Zhang2012Robust}
Zhang, S.; Zhao, X.; Lei, B.
\newblock Robust Facial Expression Recognition via Compressive Sensing.
\newblock {\em Sensors} {\bf 2012}, {\em 12},~3747--3761.

\bibitem[Cheong \em{et~al.}(2012)Cheong, Seo, and Han]{Cheong2012Advanced}
Cheong, C.; Seo, J.; Han, T.D.
\newblock Advanced contour tracing algorithms based on analysis of tracing
 conditions.
\newblock In Proceedings of the 33rd KISS Fall Conference, \hl{ City, Country, start date--end date 2012}; Volume 
 33, pp. 431--436.

\bibitem[Miyatake \em{et~al.}(1997)Miyatake, Matsushima, and
 Ejiri]{Miyatake1997Contour}
Miyatake, T.; Matsushima, H.; Ejiri, M.
\newblock Contour representation of binary images using run-type direction
 codes.
\newblock {\em Mach. Vis. Appl.} {\bf 1997}, {\em 9},~193--200.

\bibitem[Danielsson(1981)]{Danielsson1981Improvement}
Danielsson, P.E.
\newblock An improvement of Kruse's segmentation algorithm.
\newblock {\em Comput. Graph. Image Proc.} {\bf 1981}, {\em
 17},~394--396.

\bibitem[Shoji \em{et~al.}(1999)Shoji, Miyamichi, and Hirano]{Shoji1999Contour}
Shoji, K.; Miyamichi, J.; Hirano, K.
\newblock Contour following and reconstruction of binary images stored in run
 format.
\newblock {\em Syst. Comput. Jpn.} {\bf 1999}, {\em 30},~1--11.

\bibitem[Toussaint()]{Toussaint????Grids}
\hl{Toussaint, G.}
\newblock Grids, Connectivity and Contour Tracing. \hl{ the book name it belongs to; Publisher: City Country, Year}.
%Please check this reference type.

\bibitem[Suzuki and Abe(1985)]{Suzuki1985Topological}
Suzuki, S.; Abe, K.
\newblock Topological structural analysis of digitized binary images by border
 following.
\newblock {\em Comput.~Vis. Graph. Image Proc.} {\bf 1985},
 {\em 30},~32--46.

\bibitem[Ghuneim(2015)]{Ghuneim2015Contour}
Ghuneim, A.
\newblock Contour Tracing. 2015.
\newblock
 Available online: \href{http://www.imageprocessingplace.com/downloads\_V3/root\_downloads/tutorials/contour\_tracing\_Abeer\_George\_Ghuneim/index.html}
 {\nolinkurl{http://www.imageprocessingplace.com/downloads\_V3/root\_downloads/tutorials/contour\_tracing\_Abeer\_George\_Ghuneim/index.html}}.
(\hl{accessed on}).
%Please add the online resource update day month year.

\bibitem[Reddy \em{et~al.}(2012)Reddy, V., and Bhaskar]{Reddy2012Evaluation}
Reddy, P.R.; V., A.; Bhaskar, M.
\newblock Evaluation of stopping criterion in contour tracing algorithms.
\newblock {\em Int. J. Comput. Sci. Inf.
 Technol.} {\bf 2012}, {\em 3},~3888--3894.

\end{thebibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\abbreviations{Abbreviations/Nomenclature}
%
%Main text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\appendix
%\section{Appendix Title}
%
%Main text.

\end{document}

